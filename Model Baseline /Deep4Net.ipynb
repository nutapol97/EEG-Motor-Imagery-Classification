{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5437aee-2405-48db-9a8e-55b502d84345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.preprocessing import (\n",
    "    exponential_moving_standardize,\n",
    "    preprocess,\n",
    "    Preprocessor)\n",
    "from numpy import multiply\n",
    "from braindecode.datasets import MOABBDataset\n",
    "from braindecode.preprocessing import create_windows_from_events\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from braindecode.visualization import plot_confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os\n",
    "import sys\n",
    "from mne.datasets import eegbci\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mne.datasets import eegbci\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import wandb\n",
    "import os\n",
    "import neptune.new.integrations.sklearn as npt_utils\n",
    "import neptune.new as neptune\n",
    "\n",
    "\n",
    "\n",
    "os.environ['http_proxy'] = \"http://192.41.170.23:3128\"\n",
    "os.environ['https_proxy'] = \"http://192.41.170.23:3128\"\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, Deep4Net ,EEGNetv4,HybridNet,EEGInceptionMI,EEGITNet,ATCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size):\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "    y_tensor = torch.tensor(y).long()\n",
    "    dataset_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "    dl = torch.utils.data.DataLoader(dataset_tensor, batch_size=batch_size, shuffle=True)\n",
    "    return dl\n",
    "\n",
    "def early_stopping(train_loss, validation_loss, min_delta, tolerance):\n",
    "    counter = 0\n",
    "    if (validation_loss - train_loss) > min_delta:\n",
    "        counter +=1\n",
    "        if counter >= tolerance:\n",
    "          return True\n",
    "\n",
    "def extrack_dataset(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    #X2 = X[:, 7:8, :]\n",
    "    #X3= X[:, 11:12, :]\n",
    "    #(288, 22, 1125)\n",
    "    #X = np.concatenate((X2,X3), axis=1)\n",
    "    print(X.shape)\n",
    "\n",
    "    #X = X.reshape(y_shape,X.shape[2],X.shape[1])\n",
    "    return X,np.array(y_).T\n",
    "\n",
    "\n",
    "def extrack_dataset_2class(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    return X_,y_for_2class\n",
    "\n",
    "def extrack_dataset_2class_cut(dataset):\n",
    "    for x, y, window_ind in dataset:\n",
    "        x_shape = x.shape\n",
    "        y_shape = len(dataset.get_metadata().target)\n",
    "        break\n",
    "    X = np.zeros((y_shape,x_shape[0],x_shape[1]))\n",
    "    y_=[]\n",
    "    i=0\n",
    "    for x, y, window_ind in dataset:\n",
    "        X[i]=x\n",
    "        y_.append(y)\n",
    "        i+=1\n",
    "    y = np.array(y_).T\n",
    "    X_ = np.zeros((int(y_shape/2) ,x_shape[0],x_shape[1])) #for 2 class\n",
    "    y_for_2class =[]\n",
    "    j=0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_for_2class.append(y[i])\n",
    "            X_[j]=X[i]\n",
    "            j +=1\n",
    "    y_for_2class = np.array(y_for_2class).T\n",
    "    X_ = X_.reshape(y_for_2class.shape[0],X_.shape[2],X_.shape[1])\n",
    "    #100, 1, 576, 22\n",
    "    X_ = X_[:,64:320,:]\n",
    "    print(X_.shape)\n",
    "    return X_,y_for_2class\n",
    "\n",
    "\n",
    "def train(model,gpu_num,train_loader,test_loader,\n",
    "          weights_name=False,\n",
    "          optimizer = None,\n",
    "          criterion = None,\n",
    "          num_epochs=500,\n",
    "          vail_loader= None,\n",
    "          save_weights = False,\n",
    "          neptune = True,\n",
    "          lr = None\n",
    "         ):\n",
    "\n",
    "    # Train the model\n",
    "\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = [10,11]\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "\n",
    "    old_loss = 100\n",
    "    old_acc = 0\n",
    "    valid_loss_vail = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, (items, classes) in enumerate(train_loader):\n",
    "            items = Variable(items)\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "            #print(avg_pedic.shape)\n",
    "            #print(classes.shape)\n",
    "            loss = criterion(outputs, classes)\n",
    "\n",
    "            iter_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            metrics = {\"train/train_loss\": loss}\n",
    "\n",
    "            #print(loss)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        train_loss.append(iter_loss/iterations)\n",
    "\n",
    "\n",
    "        train_accuracy.append(( correct.float() / len(train_loader.dataset)))\n",
    "        train_metrics = {\"train/train_loss\": iter_loss/iterations,\n",
    "                       \"train/train_accuracy\": (100 * correct.float() / len(train_loader.dataset))}\n",
    "\n",
    "\n",
    "\n",
    "        loss = 0.0\n",
    "        correct = 0\n",
    "        iterations = 0\n",
    "        model.eval()\n",
    "\n",
    "        for i, (items, classes) in enumerate(test_loader):\n",
    "            classes = classes.type(torch.LongTensor)\n",
    "            items = Variable(items)\n",
    "            classes = Variable(classes)\n",
    "\n",
    "            if cuda.is_available():\n",
    "                items = items.cuda(gpu_num)\n",
    "                classes = classes.cuda(gpu_num)\n",
    "\n",
    "\n",
    "            outputs = model(items)\n",
    "            #avg_pedic = torch.mean(outputs, 1, True).float()\n",
    "            #avg_pedic = avg_pedic.reshape(classes.shape[0])\n",
    "\n",
    "            loss += criterion(outputs, classes).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            correct += (predicted == classes.data).sum()\n",
    "            #print(\"correct : {}\".format(classes.data))\n",
    "            #print(\"predicted : {}\".format(predicted))\n",
    "            iterations += 1\n",
    "\n",
    "        valid_loss.append(loss/iterations)\n",
    "        correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "        valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "        test_metrics = {\"Test/Test_loss\": loss/iterations,\n",
    "                       \"Test/Test_accuracy\": correct_scalar / len(test_loader.dataset) }\n",
    "        if save_weights is True:\n",
    "            if epoch+1 > 2 and valid_loss[-1] < old_loss and old_acc <= valid_accuracy[-1] :\n",
    "                    newpath = r'./{}'.format(weights_name)\n",
    "                    if not os.path.exists(newpath):\n",
    "                        os.makedirs(newpath)\n",
    "                    torch.save(model.state_dict(),'./{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1]))\n",
    "                    part_weights = './{}/{:.4f}_{}_{:.4f}_{:.4f}'.format(weights_name,valid_loss[-1],weights_name,valid_loss[-1],valid_accuracy[-1])\n",
    "                    old_loss = valid_loss[-1]\n",
    "                    old_acc = valid_accuracy[-1]\n",
    "\n",
    "        print ('Epoch %d/%d, Tr Loss: %.4f, Tr Acc: %.4f, Val Loss: %.4f, Val Acc: %.4f , le : %f'\n",
    "                       %(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1],lr))\n",
    "        if early_stopping(train_loss[-1], valid_loss[-1], min_delta=10, tolerance = 20):\n",
    "            print(\"We are at epoch:\", epoch+1)\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "            break\n",
    "        if neptune is True:\n",
    "            run[f\"epoch/valid_accuracy\"].append(valid_accuracy[-1])\n",
    "            run[f\"epoch/train_accuracy\"].append(train_accuracy[-1])\n",
    "            run[f\"epoch/train_loss\"].append(train_loss[-1])\n",
    "            run[f\"epoch/valid_loss\"].append(valid_loss[-1])\n",
    "\n",
    "\n",
    "    return train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights\n",
    "def eval(model,\n",
    "         gpu_num,\n",
    "          vail_loader= None,\n",
    "         labels=None,\n",
    "         ):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    correct=0\n",
    "    for i, (items, classes) in enumerate(vail_loader):\n",
    "        classes = classes.type(torch.LongTensor)\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        if cuda.is_available():\n",
    "            items = items.cuda(gpu_num)\n",
    "            classes = classes.cuda(gpu_num)\n",
    "\n",
    "        outputs = model(items)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted.clone().cpu().numpy())\n",
    "        y_true.append(classes.data.clone().cpu().numpy())\n",
    "        correct += (predicted == classes.data).sum()\n",
    "    correct_scalar = np.array([correct.clone().cpu()])[0]\n",
    "    valid_accuracy.append(correct_scalar / len(test_loader.dataset) )\n",
    "\n",
    "    confusion_mat = confusion_matrix(np.array(y_true).T,np.array(y_pred).T )\n",
    "    run[f\"epoch/eval_ACC\"].append(valid_accuracy[-1])\n",
    "    run[\"confusion matrices subject_id : {0}\".format(subject_id)].upload(plot_confusion_matrix(confusion_mat, class_names=labels,rotate_row_labels=0,rotate_col_labels=90,with_f1_score=True))\n",
    "    return y_pred,y_true,correct_scalar,valid_accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc9d1e3-3e9a-43e5-9db8-5cd28e492c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nutapolt/.local/lib/python3.8/site-packages/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.\n",
      "  warn('Preprocessing choices with lambda functions cannot be saved.')\n"
     ]
    }
   ],
   "source": [
    "subjects = list(range(1,10))\n",
    "\n",
    "low_cut_hz = 8.  # low cut frequency for filtering\n",
    "high_cut_hz = 35.  # high cut frequency for filtering\n",
    "resample = 128\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "# Factor to convert from V to uV\n",
    "factor = 1e6\n",
    "preprocessors = [\n",
    "                    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
    "                    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n",
    "                    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "                    #Preprocessor('resample', sfreq=resample),\n",
    "                    Preprocessor(exponential_moving_standardize,  # Exponential movin standardization\n",
    "                                factor_new=factor_new,\n",
    "                                 init_block_size=init_block_size)\n",
    "                    ]\n",
    "n_epochs = 20000\n",
    "lr = 1 * 0.00001\n",
    "weight_decay = 0.5 * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd83b5-89c5-459a-9ff4-e4f7b1e9ba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/AitBrainLab/BaseLine/e/BASELINE-205\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "48 events found\n",
      "Event IDs: [1 2 3 4]\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 413 samples (1.652 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.0\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
      "(288, 22, 1125)\n",
      "(288, 22, 1125)\n",
      "train size (201, 22, 1125) (201,)\n",
      "test size (87, 22, 1125) (87,)\n",
      "Epoch 1/20000, Tr Loss: 2.0394, Tr Acc: 0.2786, Val Loss: 1.3819, Val Acc: 0.2184 , le : 0.000100\n",
      "Epoch 2/20000, Tr Loss: 1.8872, Tr Acc: 0.2488, Val Loss: 1.3794, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 3/20000, Tr Loss: 1.8911, Tr Acc: 0.2687, Val Loss: 1.3779, Val Acc: 0.2759 , le : 0.000100\n",
      "Epoch 4/20000, Tr Loss: 1.8336, Tr Acc: 0.2935, Val Loss: 1.3788, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 5/20000, Tr Loss: 1.7008, Tr Acc: 0.2736, Val Loss: 1.3833, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 6/20000, Tr Loss: 1.8497, Tr Acc: 0.2289, Val Loss: 1.3923, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 7/20000, Tr Loss: 1.6392, Tr Acc: 0.2836, Val Loss: 1.4052, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 8/20000, Tr Loss: 1.6570, Tr Acc: 0.2637, Val Loss: 1.4225, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 9/20000, Tr Loss: 1.6624, Tr Acc: 0.2637, Val Loss: 1.4444, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 10/20000, Tr Loss: 1.6108, Tr Acc: 0.2836, Val Loss: 1.4699, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 11/20000, Tr Loss: 1.5749, Tr Acc: 0.3333, Val Loss: 1.4996, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 12/20000, Tr Loss: 1.7201, Tr Acc: 0.2637, Val Loss: 1.5319, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 13/20000, Tr Loss: 1.5894, Tr Acc: 0.3035, Val Loss: 1.5681, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 14/20000, Tr Loss: 1.5710, Tr Acc: 0.3333, Val Loss: 1.6064, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 15/20000, Tr Loss: 1.6870, Tr Acc: 0.3134, Val Loss: 1.6458, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 16/20000, Tr Loss: 1.5692, Tr Acc: 0.3085, Val Loss: 1.6874, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 17/20000, Tr Loss: 1.7293, Tr Acc: 0.2687, Val Loss: 1.7291, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 18/20000, Tr Loss: 1.5422, Tr Acc: 0.3234, Val Loss: 1.7718, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 19/20000, Tr Loss: 1.6186, Tr Acc: 0.3184, Val Loss: 1.8171, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 20/20000, Tr Loss: 1.5993, Tr Acc: 0.2736, Val Loss: 1.8644, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 21/20000, Tr Loss: 1.6310, Tr Acc: 0.2935, Val Loss: 1.9120, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 22/20000, Tr Loss: 1.6909, Tr Acc: 0.2935, Val Loss: 1.9603, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 23/20000, Tr Loss: 1.5971, Tr Acc: 0.3035, Val Loss: 2.0068, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 24/20000, Tr Loss: 1.5010, Tr Acc: 0.3284, Val Loss: 2.0523, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 25/20000, Tr Loss: 1.5694, Tr Acc: 0.2637, Val Loss: 2.1002, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 26/20000, Tr Loss: 1.4832, Tr Acc: 0.3682, Val Loss: 2.1462, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 27/20000, Tr Loss: 1.5354, Tr Acc: 0.3532, Val Loss: 2.1938, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 28/20000, Tr Loss: 1.6266, Tr Acc: 0.2886, Val Loss: 2.2398, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 29/20000, Tr Loss: 1.6398, Tr Acc: 0.3234, Val Loss: 2.2850, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 30/20000, Tr Loss: 1.5073, Tr Acc: 0.2836, Val Loss: 2.3287, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 31/20000, Tr Loss: 1.4419, Tr Acc: 0.3781, Val Loss: 2.3707, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 32/20000, Tr Loss: 1.4950, Tr Acc: 0.3582, Val Loss: 2.4120, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 33/20000, Tr Loss: 1.4200, Tr Acc: 0.4129, Val Loss: 2.4506, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 34/20000, Tr Loss: 1.4875, Tr Acc: 0.3682, Val Loss: 2.4928, Val Acc: 0.2414 , le : 0.000100\n",
      "Epoch 35/20000, Tr Loss: 1.5163, Tr Acc: 0.3532, Val Loss: 2.5352, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 36/20000, Tr Loss: 1.4325, Tr Acc: 0.3582, Val Loss: 2.5763, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 37/20000, Tr Loss: 1.4683, Tr Acc: 0.3234, Val Loss: 2.6182, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 38/20000, Tr Loss: 1.4234, Tr Acc: 0.3682, Val Loss: 2.6579, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 39/20000, Tr Loss: 1.4600, Tr Acc: 0.3532, Val Loss: 2.6974, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 40/20000, Tr Loss: 1.4185, Tr Acc: 0.3930, Val Loss: 2.7359, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 41/20000, Tr Loss: 1.3849, Tr Acc: 0.4030, Val Loss: 2.7775, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 42/20000, Tr Loss: 1.5077, Tr Acc: 0.3184, Val Loss: 2.8184, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 43/20000, Tr Loss: 1.4693, Tr Acc: 0.3731, Val Loss: 2.8613, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 44/20000, Tr Loss: 1.4550, Tr Acc: 0.3433, Val Loss: 2.9020, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 45/20000, Tr Loss: 1.4140, Tr Acc: 0.3781, Val Loss: 2.9445, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 46/20000, Tr Loss: 1.4379, Tr Acc: 0.3831, Val Loss: 2.9890, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 47/20000, Tr Loss: 1.3989, Tr Acc: 0.3532, Val Loss: 3.0324, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 48/20000, Tr Loss: 1.3774, Tr Acc: 0.3731, Val Loss: 3.0758, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 49/20000, Tr Loss: 1.3584, Tr Acc: 0.4030, Val Loss: 3.1183, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 50/20000, Tr Loss: 1.3771, Tr Acc: 0.3980, Val Loss: 3.1654, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 51/20000, Tr Loss: 1.3179, Tr Acc: 0.3980, Val Loss: 3.2063, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 52/20000, Tr Loss: 1.3976, Tr Acc: 0.3433, Val Loss: 3.2488, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 53/20000, Tr Loss: 1.2668, Tr Acc: 0.4378, Val Loss: 3.2899, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 54/20000, Tr Loss: 1.3371, Tr Acc: 0.4080, Val Loss: 3.3316, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 55/20000, Tr Loss: 1.3192, Tr Acc: 0.3831, Val Loss: 3.3707, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 56/20000, Tr Loss: 1.3838, Tr Acc: 0.4179, Val Loss: 3.4075, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 57/20000, Tr Loss: 1.2768, Tr Acc: 0.3881, Val Loss: 3.4436, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 58/20000, Tr Loss: 1.3214, Tr Acc: 0.4080, Val Loss: 3.4782, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 59/20000, Tr Loss: 1.2125, Tr Acc: 0.4627, Val Loss: 3.5165, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 60/20000, Tr Loss: 1.2940, Tr Acc: 0.4229, Val Loss: 3.5541, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 61/20000, Tr Loss: 1.2941, Tr Acc: 0.4428, Val Loss: 3.5898, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 62/20000, Tr Loss: 1.3791, Tr Acc: 0.3682, Val Loss: 3.6270, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 63/20000, Tr Loss: 1.1857, Tr Acc: 0.4726, Val Loss: 3.6627, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 64/20000, Tr Loss: 1.3661, Tr Acc: 0.3930, Val Loss: 3.6956, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 65/20000, Tr Loss: 1.1932, Tr Acc: 0.4080, Val Loss: 3.7285, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 66/20000, Tr Loss: 1.1802, Tr Acc: 0.4826, Val Loss: 3.7612, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 67/20000, Tr Loss: 1.2108, Tr Acc: 0.5075, Val Loss: 3.7949, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 68/20000, Tr Loss: 1.2165, Tr Acc: 0.4478, Val Loss: 3.8321, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 69/20000, Tr Loss: 1.2047, Tr Acc: 0.4428, Val Loss: 3.8658, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 70/20000, Tr Loss: 1.1780, Tr Acc: 0.5124, Val Loss: 3.8973, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 71/20000, Tr Loss: 1.3448, Tr Acc: 0.4030, Val Loss: 3.9304, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 72/20000, Tr Loss: 1.1649, Tr Acc: 0.4925, Val Loss: 3.9674, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 73/20000, Tr Loss: 1.1625, Tr Acc: 0.4627, Val Loss: 3.9998, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 74/20000, Tr Loss: 1.2977, Tr Acc: 0.4328, Val Loss: 4.0336, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 75/20000, Tr Loss: 1.1407, Tr Acc: 0.5075, Val Loss: 4.0638, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 76/20000, Tr Loss: 1.1679, Tr Acc: 0.4527, Val Loss: 4.0954, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 77/20000, Tr Loss: 1.1565, Tr Acc: 0.4925, Val Loss: 4.1232, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 78/20000, Tr Loss: 1.1815, Tr Acc: 0.4677, Val Loss: 4.1526, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 79/20000, Tr Loss: 1.1963, Tr Acc: 0.4876, Val Loss: 4.1748, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 80/20000, Tr Loss: 1.1885, Tr Acc: 0.4826, Val Loss: 4.2021, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 81/20000, Tr Loss: 1.0874, Tr Acc: 0.4826, Val Loss: 4.2260, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 82/20000, Tr Loss: 0.9781, Tr Acc: 0.5174, Val Loss: 4.2488, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 83/20000, Tr Loss: 1.0635, Tr Acc: 0.5124, Val Loss: 4.2723, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 84/20000, Tr Loss: 1.1726, Tr Acc: 0.4776, Val Loss: 4.2909, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 85/20000, Tr Loss: 1.0621, Tr Acc: 0.5025, Val Loss: 4.3096, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 86/20000, Tr Loss: 1.1260, Tr Acc: 0.5025, Val Loss: 4.3272, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 87/20000, Tr Loss: 1.0993, Tr Acc: 0.5174, Val Loss: 4.3419, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 88/20000, Tr Loss: 1.0561, Tr Acc: 0.5025, Val Loss: 4.3598, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 89/20000, Tr Loss: 1.0139, Tr Acc: 0.5373, Val Loss: 4.3763, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 90/20000, Tr Loss: 1.0784, Tr Acc: 0.5274, Val Loss: 4.3961, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 91/20000, Tr Loss: 1.0446, Tr Acc: 0.5373, Val Loss: 4.4158, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 92/20000, Tr Loss: 1.0097, Tr Acc: 0.5274, Val Loss: 4.4368, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 93/20000, Tr Loss: 1.0459, Tr Acc: 0.5473, Val Loss: 4.4563, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 94/20000, Tr Loss: 1.0364, Tr Acc: 0.5522, Val Loss: 4.4788, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 95/20000, Tr Loss: 1.0429, Tr Acc: 0.4975, Val Loss: 4.5013, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 96/20000, Tr Loss: 0.9920, Tr Acc: 0.5672, Val Loss: 4.5200, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 97/20000, Tr Loss: 1.1117, Tr Acc: 0.4876, Val Loss: 4.5374, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 98/20000, Tr Loss: 1.0410, Tr Acc: 0.5274, Val Loss: 4.5539, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 99/20000, Tr Loss: 1.0703, Tr Acc: 0.5124, Val Loss: 4.5750, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 100/20000, Tr Loss: 1.0375, Tr Acc: 0.5075, Val Loss: 4.5920, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 101/20000, Tr Loss: 1.0832, Tr Acc: 0.5224, Val Loss: 4.6103, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 102/20000, Tr Loss: 0.9803, Tr Acc: 0.5274, Val Loss: 4.6257, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 103/20000, Tr Loss: 1.0394, Tr Acc: 0.5323, Val Loss: 4.6437, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 104/20000, Tr Loss: 0.9922, Tr Acc: 0.5672, Val Loss: 4.6577, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 105/20000, Tr Loss: 1.0495, Tr Acc: 0.4826, Val Loss: 4.6681, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 106/20000, Tr Loss: 1.0066, Tr Acc: 0.5522, Val Loss: 4.6792, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 107/20000, Tr Loss: 1.0184, Tr Acc: 0.5323, Val Loss: 4.6932, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 108/20000, Tr Loss: 0.9937, Tr Acc: 0.5423, Val Loss: 4.7114, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 109/20000, Tr Loss: 0.9741, Tr Acc: 0.5174, Val Loss: 4.7234, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 110/20000, Tr Loss: 0.9759, Tr Acc: 0.5771, Val Loss: 4.7328, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 111/20000, Tr Loss: 0.9978, Tr Acc: 0.5174, Val Loss: 4.7444, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 112/20000, Tr Loss: 0.9399, Tr Acc: 0.5522, Val Loss: 4.7532, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 113/20000, Tr Loss: 1.0163, Tr Acc: 0.5522, Val Loss: 4.7550, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 114/20000, Tr Loss: 0.9576, Tr Acc: 0.5721, Val Loss: 4.7566, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 115/20000, Tr Loss: 1.0085, Tr Acc: 0.5522, Val Loss: 4.7541, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 116/20000, Tr Loss: 1.0053, Tr Acc: 0.5174, Val Loss: 4.7547, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 117/20000, Tr Loss: 0.8981, Tr Acc: 0.5871, Val Loss: 4.7612, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 118/20000, Tr Loss: 0.9435, Tr Acc: 0.5771, Val Loss: 4.7698, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 119/20000, Tr Loss: 0.8826, Tr Acc: 0.6318, Val Loss: 4.7781, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 120/20000, Tr Loss: 0.9284, Tr Acc: 0.5622, Val Loss: 4.7850, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 121/20000, Tr Loss: 0.9563, Tr Acc: 0.6020, Val Loss: 4.7935, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 122/20000, Tr Loss: 0.9590, Tr Acc: 0.5572, Val Loss: 4.8034, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 123/20000, Tr Loss: 0.9422, Tr Acc: 0.5771, Val Loss: 4.8166, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 124/20000, Tr Loss: 0.8732, Tr Acc: 0.5970, Val Loss: 4.8265, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 125/20000, Tr Loss: 0.9015, Tr Acc: 0.5473, Val Loss: 4.8346, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 126/20000, Tr Loss: 0.9159, Tr Acc: 0.5622, Val Loss: 4.8447, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 127/20000, Tr Loss: 0.9588, Tr Acc: 0.5871, Val Loss: 4.8516, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 128/20000, Tr Loss: 0.9307, Tr Acc: 0.5323, Val Loss: 4.8553, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 129/20000, Tr Loss: 0.9426, Tr Acc: 0.5970, Val Loss: 4.8650, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 130/20000, Tr Loss: 1.0051, Tr Acc: 0.5124, Val Loss: 4.8760, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 131/20000, Tr Loss: 0.9194, Tr Acc: 0.5970, Val Loss: 4.8801, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 132/20000, Tr Loss: 0.9645, Tr Acc: 0.5622, Val Loss: 4.8897, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 133/20000, Tr Loss: 0.9341, Tr Acc: 0.5672, Val Loss: 4.9026, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 134/20000, Tr Loss: 0.8730, Tr Acc: 0.5970, Val Loss: 4.9147, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 135/20000, Tr Loss: 0.9282, Tr Acc: 0.5473, Val Loss: 4.9258, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 136/20000, Tr Loss: 0.8669, Tr Acc: 0.6119, Val Loss: 4.9312, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 137/20000, Tr Loss: 0.8237, Tr Acc: 0.6020, Val Loss: 4.9408, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 138/20000, Tr Loss: 0.8854, Tr Acc: 0.6169, Val Loss: 4.9469, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 139/20000, Tr Loss: 0.9071, Tr Acc: 0.6119, Val Loss: 4.9508, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 140/20000, Tr Loss: 0.9788, Tr Acc: 0.5522, Val Loss: 4.9567, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 141/20000, Tr Loss: 0.9502, Tr Acc: 0.5721, Val Loss: 4.9606, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 142/20000, Tr Loss: 1.0129, Tr Acc: 0.5224, Val Loss: 4.9637, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 143/20000, Tr Loss: 0.9557, Tr Acc: 0.5721, Val Loss: 4.9577, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 144/20000, Tr Loss: 0.8860, Tr Acc: 0.5970, Val Loss: 4.9501, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 145/20000, Tr Loss: 0.9022, Tr Acc: 0.5871, Val Loss: 4.9466, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 146/20000, Tr Loss: 0.9063, Tr Acc: 0.6119, Val Loss: 4.9406, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 147/20000, Tr Loss: 0.9271, Tr Acc: 0.5672, Val Loss: 4.9324, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 148/20000, Tr Loss: 0.8544, Tr Acc: 0.6020, Val Loss: 4.9271, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 149/20000, Tr Loss: 0.9109, Tr Acc: 0.5174, Val Loss: 4.9257, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 150/20000, Tr Loss: 0.9193, Tr Acc: 0.5721, Val Loss: 4.9226, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 151/20000, Tr Loss: 0.8252, Tr Acc: 0.6070, Val Loss: 4.9241, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 152/20000, Tr Loss: 0.9226, Tr Acc: 0.6070, Val Loss: 4.9289, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 153/20000, Tr Loss: 0.8810, Tr Acc: 0.6070, Val Loss: 4.9380, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 154/20000, Tr Loss: 0.8418, Tr Acc: 0.6219, Val Loss: 4.9453, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 155/20000, Tr Loss: 0.8891, Tr Acc: 0.5672, Val Loss: 4.9561, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 156/20000, Tr Loss: 0.8378, Tr Acc: 0.6368, Val Loss: 4.9640, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 157/20000, Tr Loss: 0.8252, Tr Acc: 0.6468, Val Loss: 4.9733, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 158/20000, Tr Loss: 0.8284, Tr Acc: 0.6119, Val Loss: 4.9788, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 159/20000, Tr Loss: 0.7984, Tr Acc: 0.6269, Val Loss: 4.9841, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 160/20000, Tr Loss: 0.9667, Tr Acc: 0.5522, Val Loss: 4.9883, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 161/20000, Tr Loss: 0.8788, Tr Acc: 0.5622, Val Loss: 4.9911, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 162/20000, Tr Loss: 0.8594, Tr Acc: 0.5721, Val Loss: 4.9922, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 163/20000, Tr Loss: 0.8594, Tr Acc: 0.6269, Val Loss: 4.9970, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 164/20000, Tr Loss: 0.8169, Tr Acc: 0.6517, Val Loss: 4.9993, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 165/20000, Tr Loss: 0.8819, Tr Acc: 0.5821, Val Loss: 4.9999, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 166/20000, Tr Loss: 0.8113, Tr Acc: 0.6468, Val Loss: 5.0026, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 167/20000, Tr Loss: 0.8262, Tr Acc: 0.6020, Val Loss: 4.9994, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 168/20000, Tr Loss: 0.8325, Tr Acc: 0.5622, Val Loss: 4.9972, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 169/20000, Tr Loss: 0.8275, Tr Acc: 0.6567, Val Loss: 4.9967, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 170/20000, Tr Loss: 0.8454, Tr Acc: 0.6318, Val Loss: 5.0027, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 171/20000, Tr Loss: 0.8464, Tr Acc: 0.6070, Val Loss: 5.0019, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 172/20000, Tr Loss: 0.8736, Tr Acc: 0.5970, Val Loss: 5.0015, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 173/20000, Tr Loss: 0.7254, Tr Acc: 0.6716, Val Loss: 4.9984, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 174/20000, Tr Loss: 0.7663, Tr Acc: 0.6219, Val Loss: 4.9960, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 175/20000, Tr Loss: 0.8375, Tr Acc: 0.6517, Val Loss: 4.9921, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 176/20000, Tr Loss: 0.8492, Tr Acc: 0.6119, Val Loss: 4.9877, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 177/20000, Tr Loss: 0.8060, Tr Acc: 0.6468, Val Loss: 4.9816, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 178/20000, Tr Loss: 0.7728, Tr Acc: 0.6468, Val Loss: 4.9736, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 179/20000, Tr Loss: 0.8398, Tr Acc: 0.6169, Val Loss: 4.9712, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 180/20000, Tr Loss: 0.8218, Tr Acc: 0.6567, Val Loss: 4.9653, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 181/20000, Tr Loss: 0.8404, Tr Acc: 0.5920, Val Loss: 4.9586, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 182/20000, Tr Loss: 0.8599, Tr Acc: 0.6269, Val Loss: 4.9508, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 183/20000, Tr Loss: 0.7994, Tr Acc: 0.6318, Val Loss: 4.9423, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 184/20000, Tr Loss: 0.9132, Tr Acc: 0.5871, Val Loss: 4.9290, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 185/20000, Tr Loss: 0.8621, Tr Acc: 0.6169, Val Loss: 4.9166, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 186/20000, Tr Loss: 0.7563, Tr Acc: 0.6269, Val Loss: 4.9039, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 187/20000, Tr Loss: 0.8241, Tr Acc: 0.6468, Val Loss: 4.8934, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 188/20000, Tr Loss: 0.7991, Tr Acc: 0.6119, Val Loss: 4.8805, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 189/20000, Tr Loss: 0.7826, Tr Acc: 0.6667, Val Loss: 4.8607, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 190/20000, Tr Loss: 0.6995, Tr Acc: 0.6866, Val Loss: 4.8427, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 191/20000, Tr Loss: 0.8436, Tr Acc: 0.6020, Val Loss: 4.8351, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 192/20000, Tr Loss: 0.7421, Tr Acc: 0.6766, Val Loss: 4.8307, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 193/20000, Tr Loss: 0.7795, Tr Acc: 0.6418, Val Loss: 4.8239, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 194/20000, Tr Loss: 0.7282, Tr Acc: 0.6915, Val Loss: 4.8217, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 195/20000, Tr Loss: 0.7915, Tr Acc: 0.6567, Val Loss: 4.8186, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 196/20000, Tr Loss: 0.7660, Tr Acc: 0.6517, Val Loss: 4.8265, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 197/20000, Tr Loss: 0.7380, Tr Acc: 0.6667, Val Loss: 4.8304, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 198/20000, Tr Loss: 0.7751, Tr Acc: 0.6468, Val Loss: 4.8333, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 199/20000, Tr Loss: 0.7280, Tr Acc: 0.6667, Val Loss: 4.8389, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 200/20000, Tr Loss: 0.8058, Tr Acc: 0.6169, Val Loss: 4.8371, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 201/20000, Tr Loss: 0.8034, Tr Acc: 0.6468, Val Loss: 4.8417, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 202/20000, Tr Loss: 0.7914, Tr Acc: 0.6269, Val Loss: 4.8405, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 203/20000, Tr Loss: 0.8738, Tr Acc: 0.5970, Val Loss: 4.8418, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 204/20000, Tr Loss: 0.7723, Tr Acc: 0.6716, Val Loss: 4.8369, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 205/20000, Tr Loss: 0.7961, Tr Acc: 0.6169, Val Loss: 4.8377, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 206/20000, Tr Loss: 0.7338, Tr Acc: 0.6468, Val Loss: 4.8345, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 207/20000, Tr Loss: 0.7849, Tr Acc: 0.6567, Val Loss: 4.8268, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 208/20000, Tr Loss: 0.8068, Tr Acc: 0.6368, Val Loss: 4.8228, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 209/20000, Tr Loss: 0.7794, Tr Acc: 0.6617, Val Loss: 4.8162, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 210/20000, Tr Loss: 0.8769, Tr Acc: 0.6119, Val Loss: 4.8061, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 211/20000, Tr Loss: 0.7327, Tr Acc: 0.6567, Val Loss: 4.7965, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 212/20000, Tr Loss: 0.6932, Tr Acc: 0.6866, Val Loss: 4.7877, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 213/20000, Tr Loss: 0.7397, Tr Acc: 0.6219, Val Loss: 4.7766, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 214/20000, Tr Loss: 0.7583, Tr Acc: 0.6766, Val Loss: 4.7586, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 215/20000, Tr Loss: 0.8202, Tr Acc: 0.6119, Val Loss: 4.7413, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 216/20000, Tr Loss: 0.7176, Tr Acc: 0.6866, Val Loss: 4.7279, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 217/20000, Tr Loss: 0.7179, Tr Acc: 0.6667, Val Loss: 4.7071, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 218/20000, Tr Loss: 0.8068, Tr Acc: 0.6169, Val Loss: 4.6954, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 219/20000, Tr Loss: 0.7586, Tr Acc: 0.7015, Val Loss: 4.6846, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 220/20000, Tr Loss: 0.7516, Tr Acc: 0.6318, Val Loss: 4.6727, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 221/20000, Tr Loss: 0.7648, Tr Acc: 0.6816, Val Loss: 4.6621, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 222/20000, Tr Loss: 0.7445, Tr Acc: 0.6567, Val Loss: 4.6528, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 223/20000, Tr Loss: 0.7548, Tr Acc: 0.6567, Val Loss: 4.6474, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 224/20000, Tr Loss: 0.7504, Tr Acc: 0.6766, Val Loss: 4.6423, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 225/20000, Tr Loss: 0.7217, Tr Acc: 0.6766, Val Loss: 4.6327, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 226/20000, Tr Loss: 0.6751, Tr Acc: 0.7164, Val Loss: 4.6311, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 227/20000, Tr Loss: 0.6728, Tr Acc: 0.6915, Val Loss: 4.6285, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 228/20000, Tr Loss: 0.7324, Tr Acc: 0.6617, Val Loss: 4.6261, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 229/20000, Tr Loss: 0.6973, Tr Acc: 0.6915, Val Loss: 4.6233, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 230/20000, Tr Loss: 0.7411, Tr Acc: 0.7114, Val Loss: 4.6198, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 231/20000, Tr Loss: 0.6373, Tr Acc: 0.6965, Val Loss: 4.6091, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 232/20000, Tr Loss: 0.7082, Tr Acc: 0.7164, Val Loss: 4.6058, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 233/20000, Tr Loss: 0.7131, Tr Acc: 0.6716, Val Loss: 4.6054, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 234/20000, Tr Loss: 0.7554, Tr Acc: 0.6418, Val Loss: 4.6013, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 235/20000, Tr Loss: 0.8019, Tr Acc: 0.6617, Val Loss: 4.5925, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 236/20000, Tr Loss: 0.6635, Tr Acc: 0.7363, Val Loss: 4.5850, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 237/20000, Tr Loss: 0.7377, Tr Acc: 0.6915, Val Loss: 4.5750, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 238/20000, Tr Loss: 0.7206, Tr Acc: 0.6716, Val Loss: 4.5700, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 239/20000, Tr Loss: 0.7299, Tr Acc: 0.6915, Val Loss: 4.5607, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 240/20000, Tr Loss: 0.7566, Tr Acc: 0.6816, Val Loss: 4.5521, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 241/20000, Tr Loss: 0.7685, Tr Acc: 0.6667, Val Loss: 4.5454, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 242/20000, Tr Loss: 0.7749, Tr Acc: 0.6667, Val Loss: 4.5435, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 243/20000, Tr Loss: 0.6692, Tr Acc: 0.6517, Val Loss: 4.5406, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 244/20000, Tr Loss: 0.7631, Tr Acc: 0.6567, Val Loss: 4.5384, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 245/20000, Tr Loss: 0.7210, Tr Acc: 0.6368, Val Loss: 4.5349, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 246/20000, Tr Loss: 0.6689, Tr Acc: 0.7114, Val Loss: 4.5283, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 247/20000, Tr Loss: 0.7268, Tr Acc: 0.6617, Val Loss: 4.5208, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 248/20000, Tr Loss: 0.6672, Tr Acc: 0.6866, Val Loss: 4.5103, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 249/20000, Tr Loss: 0.7282, Tr Acc: 0.6368, Val Loss: 4.5004, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 250/20000, Tr Loss: 0.7237, Tr Acc: 0.6667, Val Loss: 4.4915, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 251/20000, Tr Loss: 0.6985, Tr Acc: 0.6965, Val Loss: 4.4812, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 252/20000, Tr Loss: 0.7127, Tr Acc: 0.6468, Val Loss: 4.4713, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 253/20000, Tr Loss: 0.6525, Tr Acc: 0.6418, Val Loss: 4.4636, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 254/20000, Tr Loss: 0.7102, Tr Acc: 0.6567, Val Loss: 4.4585, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 255/20000, Tr Loss: 0.6847, Tr Acc: 0.6915, Val Loss: 4.4502, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 256/20000, Tr Loss: 0.6939, Tr Acc: 0.6766, Val Loss: 4.4497, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 257/20000, Tr Loss: 0.7109, Tr Acc: 0.6468, Val Loss: 4.4409, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 258/20000, Tr Loss: 0.6805, Tr Acc: 0.7065, Val Loss: 4.4373, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 259/20000, Tr Loss: 0.7421, Tr Acc: 0.6617, Val Loss: 4.4295, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 260/20000, Tr Loss: 0.6569, Tr Acc: 0.7463, Val Loss: 4.4233, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 261/20000, Tr Loss: 0.7303, Tr Acc: 0.6716, Val Loss: 4.4176, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 262/20000, Tr Loss: 0.5896, Tr Acc: 0.7065, Val Loss: 4.4121, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 263/20000, Tr Loss: 0.7312, Tr Acc: 0.6418, Val Loss: 4.4022, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 264/20000, Tr Loss: 0.6627, Tr Acc: 0.7065, Val Loss: 4.3959, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 265/20000, Tr Loss: 0.7084, Tr Acc: 0.6617, Val Loss: 4.3840, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 266/20000, Tr Loss: 0.6285, Tr Acc: 0.7214, Val Loss: 4.3745, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 267/20000, Tr Loss: 0.7044, Tr Acc: 0.6816, Val Loss: 4.3596, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 268/20000, Tr Loss: 0.6366, Tr Acc: 0.7164, Val Loss: 4.3411, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 269/20000, Tr Loss: 0.6826, Tr Acc: 0.7164, Val Loss: 4.3287, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 270/20000, Tr Loss: 0.6622, Tr Acc: 0.7114, Val Loss: 4.3212, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 271/20000, Tr Loss: 0.6649, Tr Acc: 0.6816, Val Loss: 4.3093, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 272/20000, Tr Loss: 0.6794, Tr Acc: 0.6617, Val Loss: 4.2952, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 273/20000, Tr Loss: 0.6748, Tr Acc: 0.7015, Val Loss: 4.2907, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 274/20000, Tr Loss: 0.6870, Tr Acc: 0.7114, Val Loss: 4.2805, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 275/20000, Tr Loss: 0.6376, Tr Acc: 0.7214, Val Loss: 4.2662, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 276/20000, Tr Loss: 0.6939, Tr Acc: 0.6418, Val Loss: 4.2520, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 277/20000, Tr Loss: 0.6472, Tr Acc: 0.7015, Val Loss: 4.2383, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 278/20000, Tr Loss: 0.6322, Tr Acc: 0.7463, Val Loss: 4.2226, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 279/20000, Tr Loss: 0.6908, Tr Acc: 0.6866, Val Loss: 4.2107, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 280/20000, Tr Loss: 0.6647, Tr Acc: 0.7164, Val Loss: 4.2044, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 281/20000, Tr Loss: 0.7214, Tr Acc: 0.6866, Val Loss: 4.1971, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 282/20000, Tr Loss: 0.7135, Tr Acc: 0.6667, Val Loss: 4.1853, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 283/20000, Tr Loss: 0.6805, Tr Acc: 0.7363, Val Loss: 4.1705, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 284/20000, Tr Loss: 0.6726, Tr Acc: 0.6965, Val Loss: 4.1654, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 285/20000, Tr Loss: 0.6379, Tr Acc: 0.6816, Val Loss: 4.1537, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 286/20000, Tr Loss: 0.6812, Tr Acc: 0.6965, Val Loss: 4.1514, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 287/20000, Tr Loss: 0.6047, Tr Acc: 0.7413, Val Loss: 4.1472, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 288/20000, Tr Loss: 0.6782, Tr Acc: 0.6866, Val Loss: 4.1404, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 289/20000, Tr Loss: 0.6001, Tr Acc: 0.7562, Val Loss: 4.1319, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 290/20000, Tr Loss: 0.5963, Tr Acc: 0.7214, Val Loss: 4.1328, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 291/20000, Tr Loss: 0.6911, Tr Acc: 0.7065, Val Loss: 4.1253, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 292/20000, Tr Loss: 0.6651, Tr Acc: 0.7164, Val Loss: 4.1257, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 293/20000, Tr Loss: 0.6391, Tr Acc: 0.7065, Val Loss: 4.1196, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 294/20000, Tr Loss: 0.6277, Tr Acc: 0.7313, Val Loss: 4.1172, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 295/20000, Tr Loss: 0.6646, Tr Acc: 0.7114, Val Loss: 4.1109, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 296/20000, Tr Loss: 0.7098, Tr Acc: 0.6567, Val Loss: 4.1061, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 297/20000, Tr Loss: 0.6035, Tr Acc: 0.7264, Val Loss: 4.0964, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 298/20000, Tr Loss: 0.7050, Tr Acc: 0.6567, Val Loss: 4.0845, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 299/20000, Tr Loss: 0.6643, Tr Acc: 0.6915, Val Loss: 4.0734, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 300/20000, Tr Loss: 0.6954, Tr Acc: 0.6915, Val Loss: 4.0551, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 301/20000, Tr Loss: 0.6428, Tr Acc: 0.6965, Val Loss: 4.0392, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 302/20000, Tr Loss: 0.6683, Tr Acc: 0.7313, Val Loss: 4.0215, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 303/20000, Tr Loss: 0.6355, Tr Acc: 0.7313, Val Loss: 4.0082, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 304/20000, Tr Loss: 0.6353, Tr Acc: 0.7065, Val Loss: 3.9913, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 305/20000, Tr Loss: 0.6424, Tr Acc: 0.6667, Val Loss: 3.9786, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 306/20000, Tr Loss: 0.5697, Tr Acc: 0.7214, Val Loss: 3.9635, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 307/20000, Tr Loss: 0.6280, Tr Acc: 0.7264, Val Loss: 3.9468, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 308/20000, Tr Loss: 0.5688, Tr Acc: 0.7264, Val Loss: 3.9370, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 309/20000, Tr Loss: 0.6856, Tr Acc: 0.6766, Val Loss: 3.9271, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 310/20000, Tr Loss: 0.6121, Tr Acc: 0.7065, Val Loss: 3.9212, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 311/20000, Tr Loss: 0.6129, Tr Acc: 0.6866, Val Loss: 3.9159, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 312/20000, Tr Loss: 0.5893, Tr Acc: 0.7711, Val Loss: 3.9195, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 313/20000, Tr Loss: 0.6727, Tr Acc: 0.7015, Val Loss: 3.9197, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 314/20000, Tr Loss: 0.5513, Tr Acc: 0.7562, Val Loss: 3.9165, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 315/20000, Tr Loss: 0.5830, Tr Acc: 0.7264, Val Loss: 3.9214, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 316/20000, Tr Loss: 0.6469, Tr Acc: 0.7214, Val Loss: 3.9250, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 317/20000, Tr Loss: 0.5980, Tr Acc: 0.7363, Val Loss: 3.9227, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 318/20000, Tr Loss: 0.6223, Tr Acc: 0.7214, Val Loss: 3.9199, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 319/20000, Tr Loss: 0.6045, Tr Acc: 0.7463, Val Loss: 3.9126, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 320/20000, Tr Loss: 0.6085, Tr Acc: 0.7114, Val Loss: 3.9074, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 321/20000, Tr Loss: 0.6465, Tr Acc: 0.7164, Val Loss: 3.9027, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 322/20000, Tr Loss: 0.5974, Tr Acc: 0.7264, Val Loss: 3.8983, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 323/20000, Tr Loss: 0.6303, Tr Acc: 0.7164, Val Loss: 3.8941, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 324/20000, Tr Loss: 0.6114, Tr Acc: 0.7264, Val Loss: 3.8879, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 325/20000, Tr Loss: 0.6405, Tr Acc: 0.7512, Val Loss: 3.8769, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 326/20000, Tr Loss: 0.5671, Tr Acc: 0.7313, Val Loss: 3.8681, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 327/20000, Tr Loss: 0.6162, Tr Acc: 0.7065, Val Loss: 3.8588, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 328/20000, Tr Loss: 0.6270, Tr Acc: 0.7214, Val Loss: 3.8473, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 329/20000, Tr Loss: 0.5424, Tr Acc: 0.7463, Val Loss: 3.8408, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 330/20000, Tr Loss: 0.6103, Tr Acc: 0.7214, Val Loss: 3.8335, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 331/20000, Tr Loss: 0.5821, Tr Acc: 0.7363, Val Loss: 3.8238, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 332/20000, Tr Loss: 0.5644, Tr Acc: 0.7612, Val Loss: 3.8163, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 333/20000, Tr Loss: 0.5768, Tr Acc: 0.7313, Val Loss: 3.8109, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 334/20000, Tr Loss: 0.6013, Tr Acc: 0.7313, Val Loss: 3.8008, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 335/20000, Tr Loss: 0.5735, Tr Acc: 0.7463, Val Loss: 3.7884, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 336/20000, Tr Loss: 0.6299, Tr Acc: 0.7065, Val Loss: 3.7766, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 337/20000, Tr Loss: 0.5684, Tr Acc: 0.7562, Val Loss: 3.7697, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 338/20000, Tr Loss: 0.5828, Tr Acc: 0.7313, Val Loss: 3.7640, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 339/20000, Tr Loss: 0.5836, Tr Acc: 0.7264, Val Loss: 3.7591, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 340/20000, Tr Loss: 0.5587, Tr Acc: 0.7562, Val Loss: 3.7520, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 341/20000, Tr Loss: 0.6126, Tr Acc: 0.7114, Val Loss: 3.7434, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 342/20000, Tr Loss: 0.5714, Tr Acc: 0.7313, Val Loss: 3.7415, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 343/20000, Tr Loss: 0.6525, Tr Acc: 0.7264, Val Loss: 3.7343, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 344/20000, Tr Loss: 0.6443, Tr Acc: 0.6866, Val Loss: 3.7240, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 345/20000, Tr Loss: 0.5660, Tr Acc: 0.7413, Val Loss: 3.7190, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 346/20000, Tr Loss: 0.5199, Tr Acc: 0.7960, Val Loss: 3.7170, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 347/20000, Tr Loss: 0.5032, Tr Acc: 0.7662, Val Loss: 3.7034, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 348/20000, Tr Loss: 0.5775, Tr Acc: 0.7363, Val Loss: 3.6959, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 349/20000, Tr Loss: 0.6294, Tr Acc: 0.7313, Val Loss: 3.6829, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 350/20000, Tr Loss: 0.5632, Tr Acc: 0.7313, Val Loss: 3.6733, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 351/20000, Tr Loss: 0.5708, Tr Acc: 0.7363, Val Loss: 3.6694, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 352/20000, Tr Loss: 0.5796, Tr Acc: 0.7761, Val Loss: 3.6613, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 353/20000, Tr Loss: 0.5831, Tr Acc: 0.7512, Val Loss: 3.6480, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 354/20000, Tr Loss: 0.6766, Tr Acc: 0.7662, Val Loss: 3.6355, Val Acc: 0.2529 , le : 0.000100\n",
      "Epoch 355/20000, Tr Loss: 0.5723, Tr Acc: 0.7214, Val Loss: 3.6172, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 356/20000, Tr Loss: 0.5907, Tr Acc: 0.7214, Val Loss: 3.5940, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 357/20000, Tr Loss: 0.5268, Tr Acc: 0.8010, Val Loss: 3.5766, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 358/20000, Tr Loss: 0.6072, Tr Acc: 0.7264, Val Loss: 3.5693, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 359/20000, Tr Loss: 0.5251, Tr Acc: 0.7413, Val Loss: 3.5610, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 360/20000, Tr Loss: 0.5894, Tr Acc: 0.7413, Val Loss: 3.5484, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 361/20000, Tr Loss: 0.5457, Tr Acc: 0.7463, Val Loss: 3.5417, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 362/20000, Tr Loss: 0.5790, Tr Acc: 0.7512, Val Loss: 3.5379, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 363/20000, Tr Loss: 0.5537, Tr Acc: 0.7662, Val Loss: 3.5371, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 364/20000, Tr Loss: 0.5773, Tr Acc: 0.7413, Val Loss: 3.5421, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 365/20000, Tr Loss: 0.5213, Tr Acc: 0.7711, Val Loss: 3.5499, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 366/20000, Tr Loss: 0.5973, Tr Acc: 0.7463, Val Loss: 3.5551, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 367/20000, Tr Loss: 0.5892, Tr Acc: 0.7214, Val Loss: 3.5585, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 368/20000, Tr Loss: 0.6056, Tr Acc: 0.7363, Val Loss: 3.5578, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 369/20000, Tr Loss: 0.6316, Tr Acc: 0.7264, Val Loss: 3.5575, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 370/20000, Tr Loss: 0.5709, Tr Acc: 0.7214, Val Loss: 3.5530, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 371/20000, Tr Loss: 0.5292, Tr Acc: 0.8060, Val Loss: 3.5472, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 372/20000, Tr Loss: 0.5425, Tr Acc: 0.7711, Val Loss: 3.5428, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 373/20000, Tr Loss: 0.5555, Tr Acc: 0.7264, Val Loss: 3.5353, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 374/20000, Tr Loss: 0.5133, Tr Acc: 0.7562, Val Loss: 3.5274, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 375/20000, Tr Loss: 0.4909, Tr Acc: 0.7811, Val Loss: 3.5213, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 376/20000, Tr Loss: 0.5209, Tr Acc: 0.7463, Val Loss: 3.5229, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 377/20000, Tr Loss: 0.5376, Tr Acc: 0.7562, Val Loss: 3.5234, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 378/20000, Tr Loss: 0.4667, Tr Acc: 0.7761, Val Loss: 3.5208, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 379/20000, Tr Loss: 0.5969, Tr Acc: 0.7562, Val Loss: 3.5188, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 380/20000, Tr Loss: 0.5618, Tr Acc: 0.7562, Val Loss: 3.5159, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 381/20000, Tr Loss: 0.6302, Tr Acc: 0.6965, Val Loss: 3.5127, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 382/20000, Tr Loss: 0.5385, Tr Acc: 0.7711, Val Loss: 3.5096, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 383/20000, Tr Loss: 0.5302, Tr Acc: 0.7711, Val Loss: 3.5065, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 384/20000, Tr Loss: 0.5538, Tr Acc: 0.7761, Val Loss: 3.4973, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 385/20000, Tr Loss: 0.5575, Tr Acc: 0.7662, Val Loss: 3.4844, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 386/20000, Tr Loss: 0.5826, Tr Acc: 0.7512, Val Loss: 3.4740, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 387/20000, Tr Loss: 0.5479, Tr Acc: 0.7711, Val Loss: 3.4602, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 388/20000, Tr Loss: 0.5164, Tr Acc: 0.7512, Val Loss: 3.4523, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 389/20000, Tr Loss: 0.5556, Tr Acc: 0.7413, Val Loss: 3.4444, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 390/20000, Tr Loss: 0.6338, Tr Acc: 0.7413, Val Loss: 3.4432, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 391/20000, Tr Loss: 0.5799, Tr Acc: 0.7164, Val Loss: 3.4416, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 392/20000, Tr Loss: 0.5441, Tr Acc: 0.8010, Val Loss: 3.4412, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 393/20000, Tr Loss: 0.5415, Tr Acc: 0.7413, Val Loss: 3.4377, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 394/20000, Tr Loss: 0.5172, Tr Acc: 0.7463, Val Loss: 3.4376, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 395/20000, Tr Loss: 0.5282, Tr Acc: 0.7463, Val Loss: 3.4365, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 396/20000, Tr Loss: 0.5902, Tr Acc: 0.7214, Val Loss: 3.4246, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 397/20000, Tr Loss: 0.5011, Tr Acc: 0.7612, Val Loss: 3.4132, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 398/20000, Tr Loss: 0.5586, Tr Acc: 0.7562, Val Loss: 3.4031, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 399/20000, Tr Loss: 0.5854, Tr Acc: 0.7363, Val Loss: 3.3880, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 400/20000, Tr Loss: 0.4317, Tr Acc: 0.8308, Val Loss: 3.3770, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 401/20000, Tr Loss: 0.5902, Tr Acc: 0.7512, Val Loss: 3.3717, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 402/20000, Tr Loss: 0.5286, Tr Acc: 0.7761, Val Loss: 3.3661, Val Acc: 0.2644 , le : 0.000100\n",
      "Epoch 403/20000, Tr Loss: 0.5887, Tr Acc: 0.7612, Val Loss: 3.3568, Val Acc: 0.2759 , le : 0.000100\n",
      "Epoch 404/20000, Tr Loss: 0.5136, Tr Acc: 0.8010, Val Loss: 3.3449, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 405/20000, Tr Loss: 0.4957, Tr Acc: 0.7861, Val Loss: 3.3284, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 406/20000, Tr Loss: 0.5013, Tr Acc: 0.7711, Val Loss: 3.3141, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 407/20000, Tr Loss: 0.5033, Tr Acc: 0.7960, Val Loss: 3.3082, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 408/20000, Tr Loss: 0.5583, Tr Acc: 0.7612, Val Loss: 3.2965, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 409/20000, Tr Loss: 0.5226, Tr Acc: 0.7861, Val Loss: 3.2849, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 410/20000, Tr Loss: 0.5151, Tr Acc: 0.7711, Val Loss: 3.2793, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 411/20000, Tr Loss: 0.5854, Tr Acc: 0.7761, Val Loss: 3.2764, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 412/20000, Tr Loss: 0.4781, Tr Acc: 0.8159, Val Loss: 3.2777, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 413/20000, Tr Loss: 0.4776, Tr Acc: 0.8010, Val Loss: 3.2772, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 414/20000, Tr Loss: 0.5772, Tr Acc: 0.7761, Val Loss: 3.2800, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 415/20000, Tr Loss: 0.5295, Tr Acc: 0.7711, Val Loss: 3.2837, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 416/20000, Tr Loss: 0.6174, Tr Acc: 0.7363, Val Loss: 3.2799, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 417/20000, Tr Loss: 0.5337, Tr Acc: 0.8109, Val Loss: 3.2846, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 418/20000, Tr Loss: 0.5109, Tr Acc: 0.7761, Val Loss: 3.2903, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 419/20000, Tr Loss: 0.5502, Tr Acc: 0.7761, Val Loss: 3.2901, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 420/20000, Tr Loss: 0.5854, Tr Acc: 0.7512, Val Loss: 3.3032, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 421/20000, Tr Loss: 0.5041, Tr Acc: 0.7662, Val Loss: 3.3067, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 422/20000, Tr Loss: 0.4810, Tr Acc: 0.7861, Val Loss: 3.3085, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 423/20000, Tr Loss: 0.4761, Tr Acc: 0.7960, Val Loss: 3.3057, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 424/20000, Tr Loss: 0.5386, Tr Acc: 0.7711, Val Loss: 3.3078, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 425/20000, Tr Loss: 0.4866, Tr Acc: 0.8109, Val Loss: 3.3096, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 426/20000, Tr Loss: 0.5122, Tr Acc: 0.7861, Val Loss: 3.3051, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 427/20000, Tr Loss: 0.5333, Tr Acc: 0.7711, Val Loss: 3.3021, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 428/20000, Tr Loss: 0.4999, Tr Acc: 0.7811, Val Loss: 3.2983, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 429/20000, Tr Loss: 0.5454, Tr Acc: 0.7761, Val Loss: 3.2942, Val Acc: 0.2874 , le : 0.000100\n",
      "Epoch 430/20000, Tr Loss: 0.4791, Tr Acc: 0.7811, Val Loss: 3.2891, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 431/20000, Tr Loss: 0.5302, Tr Acc: 0.7711, Val Loss: 3.2892, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 432/20000, Tr Loss: 0.5133, Tr Acc: 0.7562, Val Loss: 3.2753, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 433/20000, Tr Loss: 0.5367, Tr Acc: 0.7960, Val Loss: 3.2653, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 434/20000, Tr Loss: 0.5192, Tr Acc: 0.7761, Val Loss: 3.2556, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 435/20000, Tr Loss: 0.4918, Tr Acc: 0.7811, Val Loss: 3.2464, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 436/20000, Tr Loss: 0.5525, Tr Acc: 0.7612, Val Loss: 3.2417, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 437/20000, Tr Loss: 0.4887, Tr Acc: 0.7811, Val Loss: 3.2345, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 438/20000, Tr Loss: 0.5093, Tr Acc: 0.7512, Val Loss: 3.2308, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 439/20000, Tr Loss: 0.5050, Tr Acc: 0.7562, Val Loss: 3.2249, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 440/20000, Tr Loss: 0.4705, Tr Acc: 0.8060, Val Loss: 3.2154, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 441/20000, Tr Loss: 0.5197, Tr Acc: 0.7413, Val Loss: 3.2060, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 442/20000, Tr Loss: 0.4618, Tr Acc: 0.8358, Val Loss: 3.2015, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 443/20000, Tr Loss: 0.5571, Tr Acc: 0.7861, Val Loss: 3.1914, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 444/20000, Tr Loss: 0.4698, Tr Acc: 0.8209, Val Loss: 3.1837, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 445/20000, Tr Loss: 0.5194, Tr Acc: 0.7811, Val Loss: 3.1756, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 446/20000, Tr Loss: 0.4800, Tr Acc: 0.7562, Val Loss: 3.1703, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 447/20000, Tr Loss: 0.4893, Tr Acc: 0.7662, Val Loss: 3.1638, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 448/20000, Tr Loss: 0.5232, Tr Acc: 0.7960, Val Loss: 3.1572, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 449/20000, Tr Loss: 0.4607, Tr Acc: 0.8259, Val Loss: 3.1522, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 450/20000, Tr Loss: 0.4437, Tr Acc: 0.8109, Val Loss: 3.1482, Val Acc: 0.2989 , le : 0.000100\n",
      "Epoch 451/20000, Tr Loss: 0.5150, Tr Acc: 0.7910, Val Loss: 3.1439, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 452/20000, Tr Loss: 0.4945, Tr Acc: 0.7562, Val Loss: 3.1423, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 453/20000, Tr Loss: 0.4815, Tr Acc: 0.7960, Val Loss: 3.1453, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 454/20000, Tr Loss: 0.4857, Tr Acc: 0.7612, Val Loss: 3.1484, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 455/20000, Tr Loss: 0.4245, Tr Acc: 0.8209, Val Loss: 3.1565, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 456/20000, Tr Loss: 0.4948, Tr Acc: 0.8109, Val Loss: 3.1568, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 457/20000, Tr Loss: 0.4882, Tr Acc: 0.7761, Val Loss: 3.1559, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 458/20000, Tr Loss: 0.5050, Tr Acc: 0.7363, Val Loss: 3.1520, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 459/20000, Tr Loss: 0.4619, Tr Acc: 0.8010, Val Loss: 3.1461, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 460/20000, Tr Loss: 0.4553, Tr Acc: 0.8010, Val Loss: 3.1379, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 461/20000, Tr Loss: 0.4891, Tr Acc: 0.8010, Val Loss: 3.1320, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 462/20000, Tr Loss: 0.4551, Tr Acc: 0.8159, Val Loss: 3.1276, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 463/20000, Tr Loss: 0.4596, Tr Acc: 0.7811, Val Loss: 3.1143, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 464/20000, Tr Loss: 0.5453, Tr Acc: 0.7512, Val Loss: 3.1036, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 465/20000, Tr Loss: 0.4853, Tr Acc: 0.7761, Val Loss: 3.0982, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 466/20000, Tr Loss: 0.4388, Tr Acc: 0.8010, Val Loss: 3.0814, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 467/20000, Tr Loss: 0.4930, Tr Acc: 0.7612, Val Loss: 3.0691, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 468/20000, Tr Loss: 0.4969, Tr Acc: 0.7910, Val Loss: 3.0557, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 469/20000, Tr Loss: 0.4343, Tr Acc: 0.7960, Val Loss: 3.0437, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 470/20000, Tr Loss: 0.4293, Tr Acc: 0.7910, Val Loss: 3.0397, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 471/20000, Tr Loss: 0.4679, Tr Acc: 0.7910, Val Loss: 3.0347, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 472/20000, Tr Loss: 0.4789, Tr Acc: 0.8060, Val Loss: 3.0344, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 473/20000, Tr Loss: 0.4088, Tr Acc: 0.8209, Val Loss: 3.0318, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 474/20000, Tr Loss: 0.4465, Tr Acc: 0.8010, Val Loss: 3.0254, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 475/20000, Tr Loss: 0.4667, Tr Acc: 0.8109, Val Loss: 3.0202, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 476/20000, Tr Loss: 0.4733, Tr Acc: 0.7910, Val Loss: 3.0175, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 477/20000, Tr Loss: 0.3992, Tr Acc: 0.8458, Val Loss: 3.0113, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 478/20000, Tr Loss: 0.5123, Tr Acc: 0.7811, Val Loss: 3.0096, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 479/20000, Tr Loss: 0.5149, Tr Acc: 0.7761, Val Loss: 3.0064, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 480/20000, Tr Loss: 0.4707, Tr Acc: 0.7711, Val Loss: 3.0039, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 481/20000, Tr Loss: 0.4821, Tr Acc: 0.7761, Val Loss: 3.0025, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 482/20000, Tr Loss: 0.4758, Tr Acc: 0.7861, Val Loss: 3.0043, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 483/20000, Tr Loss: 0.5225, Tr Acc: 0.7761, Val Loss: 3.0030, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 484/20000, Tr Loss: 0.5070, Tr Acc: 0.8109, Val Loss: 2.9994, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 485/20000, Tr Loss: 0.4285, Tr Acc: 0.8159, Val Loss: 2.9916, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 486/20000, Tr Loss: 0.4232, Tr Acc: 0.8209, Val Loss: 2.9868, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 487/20000, Tr Loss: 0.4002, Tr Acc: 0.8358, Val Loss: 2.9822, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 488/20000, Tr Loss: 0.5168, Tr Acc: 0.7562, Val Loss: 2.9797, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 489/20000, Tr Loss: 0.4104, Tr Acc: 0.8358, Val Loss: 2.9743, Val Acc: 0.3103 , le : 0.000100\n",
      "Epoch 490/20000, Tr Loss: 0.4224, Tr Acc: 0.7960, Val Loss: 2.9681, Val Acc: 0.3218 , le : 0.000100\n",
      "Epoch 491/20000, Tr Loss: 0.4576, Tr Acc: 0.7960, Val Loss: 2.9723, Val Acc: 0.3333 , le : 0.000100\n",
      "Epoch 492/20000, Tr Loss: 0.5114, Tr Acc: 0.7363, Val Loss: 2.9714, Val Acc: 0.3333 , le : 0.000100\n",
      "Epoch 493/20000, Tr Loss: 0.4391, Tr Acc: 0.8259, Val Loss: 2.9724, Val Acc: 0.3333 , le : 0.000100\n",
      "Epoch 494/20000, Tr Loss: 0.4849, Tr Acc: 0.7711, Val Loss: 2.9645, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 495/20000, Tr Loss: 0.4387, Tr Acc: 0.8159, Val Loss: 2.9602, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 496/20000, Tr Loss: 0.4103, Tr Acc: 0.8159, Val Loss: 2.9557, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 497/20000, Tr Loss: 0.4802, Tr Acc: 0.7960, Val Loss: 2.9508, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 498/20000, Tr Loss: 0.4469, Tr Acc: 0.8209, Val Loss: 2.9458, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 499/20000, Tr Loss: 0.4986, Tr Acc: 0.7861, Val Loss: 2.9399, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 500/20000, Tr Loss: 0.3805, Tr Acc: 0.8358, Val Loss: 2.9348, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 501/20000, Tr Loss: 0.4481, Tr Acc: 0.7761, Val Loss: 2.9299, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 502/20000, Tr Loss: 0.4367, Tr Acc: 0.8259, Val Loss: 2.9283, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 503/20000, Tr Loss: 0.4619, Tr Acc: 0.8109, Val Loss: 2.9300, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 504/20000, Tr Loss: 0.4850, Tr Acc: 0.7662, Val Loss: 2.9292, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 505/20000, Tr Loss: 0.4869, Tr Acc: 0.8209, Val Loss: 2.9324, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 506/20000, Tr Loss: 0.4001, Tr Acc: 0.8458, Val Loss: 2.9320, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 507/20000, Tr Loss: 0.4853, Tr Acc: 0.8010, Val Loss: 2.9311, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 508/20000, Tr Loss: 0.4118, Tr Acc: 0.8507, Val Loss: 2.9274, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 509/20000, Tr Loss: 0.4241, Tr Acc: 0.8109, Val Loss: 2.9265, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 510/20000, Tr Loss: 0.4523, Tr Acc: 0.7910, Val Loss: 2.9329, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 511/20000, Tr Loss: 0.4088, Tr Acc: 0.8159, Val Loss: 2.9302, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 512/20000, Tr Loss: 0.5202, Tr Acc: 0.7711, Val Loss: 2.9252, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 513/20000, Tr Loss: 0.4035, Tr Acc: 0.8010, Val Loss: 2.9217, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 514/20000, Tr Loss: 0.4469, Tr Acc: 0.8358, Val Loss: 2.9141, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 515/20000, Tr Loss: 0.4031, Tr Acc: 0.8259, Val Loss: 2.9075, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 516/20000, Tr Loss: 0.4172, Tr Acc: 0.8308, Val Loss: 2.8995, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 517/20000, Tr Loss: 0.3613, Tr Acc: 0.8607, Val Loss: 2.8927, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 518/20000, Tr Loss: 0.3778, Tr Acc: 0.8557, Val Loss: 2.8869, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 519/20000, Tr Loss: 0.3865, Tr Acc: 0.8458, Val Loss: 2.8757, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 520/20000, Tr Loss: 0.3965, Tr Acc: 0.8209, Val Loss: 2.8664, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 521/20000, Tr Loss: 0.4655, Tr Acc: 0.7910, Val Loss: 2.8574, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 522/20000, Tr Loss: 0.4194, Tr Acc: 0.8308, Val Loss: 2.8519, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 523/20000, Tr Loss: 0.4386, Tr Acc: 0.8159, Val Loss: 2.8477, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 524/20000, Tr Loss: 0.3859, Tr Acc: 0.8607, Val Loss: 2.8452, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 525/20000, Tr Loss: 0.4285, Tr Acc: 0.8060, Val Loss: 2.8405, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 526/20000, Tr Loss: 0.3424, Tr Acc: 0.8507, Val Loss: 2.8444, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 527/20000, Tr Loss: 0.3746, Tr Acc: 0.8209, Val Loss: 2.8518, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 528/20000, Tr Loss: 0.3855, Tr Acc: 0.8408, Val Loss: 2.8532, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 529/20000, Tr Loss: 0.4427, Tr Acc: 0.8159, Val Loss: 2.8552, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 530/20000, Tr Loss: 0.4189, Tr Acc: 0.8259, Val Loss: 2.8544, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 531/20000, Tr Loss: 0.3908, Tr Acc: 0.8458, Val Loss: 2.8509, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 532/20000, Tr Loss: 0.5142, Tr Acc: 0.7662, Val Loss: 2.8447, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 533/20000, Tr Loss: 0.4111, Tr Acc: 0.8159, Val Loss: 2.8399, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 534/20000, Tr Loss: 0.3802, Tr Acc: 0.8358, Val Loss: 2.8344, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 535/20000, Tr Loss: 0.4168, Tr Acc: 0.8259, Val Loss: 2.8264, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 536/20000, Tr Loss: 0.4233, Tr Acc: 0.8060, Val Loss: 2.8121, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 537/20000, Tr Loss: 0.3882, Tr Acc: 0.8408, Val Loss: 2.8028, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 538/20000, Tr Loss: 0.4238, Tr Acc: 0.8060, Val Loss: 2.7955, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 539/20000, Tr Loss: 0.4342, Tr Acc: 0.8259, Val Loss: 2.7931, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 540/20000, Tr Loss: 0.4319, Tr Acc: 0.8209, Val Loss: 2.7881, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 541/20000, Tr Loss: 0.4242, Tr Acc: 0.8259, Val Loss: 2.7852, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 542/20000, Tr Loss: 0.4947, Tr Acc: 0.7711, Val Loss: 2.7841, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 543/20000, Tr Loss: 0.4498, Tr Acc: 0.8109, Val Loss: 2.7863, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 544/20000, Tr Loss: 0.3770, Tr Acc: 0.8557, Val Loss: 2.7843, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 545/20000, Tr Loss: 0.3890, Tr Acc: 0.8259, Val Loss: 2.7831, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 546/20000, Tr Loss: 0.3826, Tr Acc: 0.8706, Val Loss: 2.7929, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 547/20000, Tr Loss: 0.4610, Tr Acc: 0.7612, Val Loss: 2.7932, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 548/20000, Tr Loss: 0.4494, Tr Acc: 0.8109, Val Loss: 2.7949, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 549/20000, Tr Loss: 0.4246, Tr Acc: 0.8109, Val Loss: 2.7966, Val Acc: 0.3448 , le : 0.000100\n",
      "Epoch 550/20000, Tr Loss: 0.3986, Tr Acc: 0.8408, Val Loss: 2.7888, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 551/20000, Tr Loss: 0.3982, Tr Acc: 0.8159, Val Loss: 2.7796, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 552/20000, Tr Loss: 0.3942, Tr Acc: 0.8458, Val Loss: 2.7653, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 553/20000, Tr Loss: 0.4238, Tr Acc: 0.8209, Val Loss: 2.7573, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 554/20000, Tr Loss: 0.3874, Tr Acc: 0.8308, Val Loss: 2.7523, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 555/20000, Tr Loss: 0.3968, Tr Acc: 0.8408, Val Loss: 2.7469, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 556/20000, Tr Loss: 0.3741, Tr Acc: 0.8657, Val Loss: 2.7430, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 557/20000, Tr Loss: 0.3925, Tr Acc: 0.8259, Val Loss: 2.7482, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 558/20000, Tr Loss: 0.3520, Tr Acc: 0.8408, Val Loss: 2.7480, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 559/20000, Tr Loss: 0.4407, Tr Acc: 0.8060, Val Loss: 2.7560, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 560/20000, Tr Loss: 0.4580, Tr Acc: 0.8159, Val Loss: 2.7573, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 561/20000, Tr Loss: 0.3674, Tr Acc: 0.8507, Val Loss: 2.7619, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 562/20000, Tr Loss: 0.4317, Tr Acc: 0.8209, Val Loss: 2.7691, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 563/20000, Tr Loss: 0.4572, Tr Acc: 0.8010, Val Loss: 2.7735, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 564/20000, Tr Loss: 0.3983, Tr Acc: 0.8159, Val Loss: 2.7778, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 565/20000, Tr Loss: 0.4093, Tr Acc: 0.8209, Val Loss: 2.7804, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 566/20000, Tr Loss: 0.3602, Tr Acc: 0.8308, Val Loss: 2.7776, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 567/20000, Tr Loss: 0.3585, Tr Acc: 0.8557, Val Loss: 2.7780, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 568/20000, Tr Loss: 0.3837, Tr Acc: 0.8458, Val Loss: 2.7771, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 569/20000, Tr Loss: 0.4218, Tr Acc: 0.8109, Val Loss: 2.7758, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 570/20000, Tr Loss: 0.3692, Tr Acc: 0.8408, Val Loss: 2.7741, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 571/20000, Tr Loss: 0.3873, Tr Acc: 0.8259, Val Loss: 2.7731, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 572/20000, Tr Loss: 0.3589, Tr Acc: 0.8358, Val Loss: 2.7724, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 573/20000, Tr Loss: 0.3708, Tr Acc: 0.8458, Val Loss: 2.7735, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 574/20000, Tr Loss: 0.4307, Tr Acc: 0.8259, Val Loss: 2.7771, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 575/20000, Tr Loss: 0.3661, Tr Acc: 0.8557, Val Loss: 2.7767, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 576/20000, Tr Loss: 0.4250, Tr Acc: 0.8358, Val Loss: 2.7723, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 577/20000, Tr Loss: 0.4187, Tr Acc: 0.8308, Val Loss: 2.7648, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 578/20000, Tr Loss: 0.4170, Tr Acc: 0.8259, Val Loss: 2.7501, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 579/20000, Tr Loss: 0.3658, Tr Acc: 0.8557, Val Loss: 2.7377, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 580/20000, Tr Loss: 0.4120, Tr Acc: 0.8358, Val Loss: 2.7246, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 581/20000, Tr Loss: 0.3722, Tr Acc: 0.8507, Val Loss: 2.7084, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 582/20000, Tr Loss: 0.3550, Tr Acc: 0.8557, Val Loss: 2.6959, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 583/20000, Tr Loss: 0.4121, Tr Acc: 0.8308, Val Loss: 2.6829, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 584/20000, Tr Loss: 0.3772, Tr Acc: 0.8358, Val Loss: 2.6719, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 585/20000, Tr Loss: 0.3692, Tr Acc: 0.8607, Val Loss: 2.6615, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 586/20000, Tr Loss: 0.3134, Tr Acc: 0.8856, Val Loss: 2.6501, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 587/20000, Tr Loss: 0.3245, Tr Acc: 0.8557, Val Loss: 2.6402, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 588/20000, Tr Loss: 0.3907, Tr Acc: 0.8557, Val Loss: 2.6316, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 589/20000, Tr Loss: 0.3989, Tr Acc: 0.8308, Val Loss: 2.6242, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 590/20000, Tr Loss: 0.3903, Tr Acc: 0.8308, Val Loss: 2.6206, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 591/20000, Tr Loss: 0.4069, Tr Acc: 0.8060, Val Loss: 2.6128, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 592/20000, Tr Loss: 0.4015, Tr Acc: 0.8060, Val Loss: 2.6120, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 593/20000, Tr Loss: 0.3670, Tr Acc: 0.8458, Val Loss: 2.6144, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 594/20000, Tr Loss: 0.3792, Tr Acc: 0.8010, Val Loss: 2.6159, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 595/20000, Tr Loss: 0.3661, Tr Acc: 0.8756, Val Loss: 2.6154, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 596/20000, Tr Loss: 0.3498, Tr Acc: 0.8607, Val Loss: 2.6199, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 597/20000, Tr Loss: 0.3390, Tr Acc: 0.8856, Val Loss: 2.6268, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 598/20000, Tr Loss: 0.3851, Tr Acc: 0.8507, Val Loss: 2.6266, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 599/20000, Tr Loss: 0.3668, Tr Acc: 0.8408, Val Loss: 2.6282, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 600/20000, Tr Loss: 0.4120, Tr Acc: 0.8010, Val Loss: 2.6285, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 601/20000, Tr Loss: 0.3835, Tr Acc: 0.8358, Val Loss: 2.6291, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 602/20000, Tr Loss: 0.3822, Tr Acc: 0.8308, Val Loss: 2.6320, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 603/20000, Tr Loss: 0.3418, Tr Acc: 0.8657, Val Loss: 2.6410, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 604/20000, Tr Loss: 0.3356, Tr Acc: 0.8806, Val Loss: 2.6504, Val Acc: 0.3563 , le : 0.000100\n",
      "Epoch 605/20000, Tr Loss: 0.3443, Tr Acc: 0.8607, Val Loss: 2.6535, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 606/20000, Tr Loss: 0.3987, Tr Acc: 0.8308, Val Loss: 2.6561, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 607/20000, Tr Loss: 0.3316, Tr Acc: 0.8557, Val Loss: 2.6548, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 608/20000, Tr Loss: 0.3343, Tr Acc: 0.8856, Val Loss: 2.6473, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 609/20000, Tr Loss: 0.3876, Tr Acc: 0.8458, Val Loss: 2.6405, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 610/20000, Tr Loss: 0.3864, Tr Acc: 0.8308, Val Loss: 2.6384, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 611/20000, Tr Loss: 0.3204, Tr Acc: 0.8657, Val Loss: 2.6322, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 612/20000, Tr Loss: 0.4235, Tr Acc: 0.8060, Val Loss: 2.6296, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 613/20000, Tr Loss: 0.3190, Tr Acc: 0.8706, Val Loss: 2.6278, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 614/20000, Tr Loss: 0.3169, Tr Acc: 0.8706, Val Loss: 2.6288, Val Acc: 0.3678 , le : 0.000100\n",
      "Epoch 615/20000, Tr Loss: 0.3367, Tr Acc: 0.8657, Val Loss: 2.6263, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 616/20000, Tr Loss: 0.3938, Tr Acc: 0.8308, Val Loss: 2.6220, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 617/20000, Tr Loss: 0.3343, Tr Acc: 0.8756, Val Loss: 2.6184, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 618/20000, Tr Loss: 0.3704, Tr Acc: 0.8458, Val Loss: 2.6185, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 619/20000, Tr Loss: 0.3966, Tr Acc: 0.8358, Val Loss: 2.6158, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 620/20000, Tr Loss: 0.3700, Tr Acc: 0.8308, Val Loss: 2.6143, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 621/20000, Tr Loss: 0.3778, Tr Acc: 0.8507, Val Loss: 2.6105, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 622/20000, Tr Loss: 0.3705, Tr Acc: 0.8358, Val Loss: 2.6131, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 623/20000, Tr Loss: 0.3397, Tr Acc: 0.8706, Val Loss: 2.6076, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 624/20000, Tr Loss: 0.3810, Tr Acc: 0.8358, Val Loss: 2.6010, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 625/20000, Tr Loss: 0.3886, Tr Acc: 0.8209, Val Loss: 2.5919, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 626/20000, Tr Loss: 0.3071, Tr Acc: 0.8955, Val Loss: 2.5833, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 627/20000, Tr Loss: 0.3715, Tr Acc: 0.8408, Val Loss: 2.5820, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 628/20000, Tr Loss: 0.3992, Tr Acc: 0.8408, Val Loss: 2.5812, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 629/20000, Tr Loss: 0.3099, Tr Acc: 0.8856, Val Loss: 2.5798, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 630/20000, Tr Loss: 0.3024, Tr Acc: 0.8905, Val Loss: 2.5790, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 631/20000, Tr Loss: 0.3327, Tr Acc: 0.8607, Val Loss: 2.5903, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 632/20000, Tr Loss: 0.3575, Tr Acc: 0.8657, Val Loss: 2.5941, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 633/20000, Tr Loss: 0.3427, Tr Acc: 0.8607, Val Loss: 2.5934, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 634/20000, Tr Loss: 0.3322, Tr Acc: 0.8806, Val Loss: 2.5906, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 635/20000, Tr Loss: 0.3554, Tr Acc: 0.8706, Val Loss: 2.5939, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 636/20000, Tr Loss: 0.3754, Tr Acc: 0.8458, Val Loss: 2.5932, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 637/20000, Tr Loss: 0.3392, Tr Acc: 0.8458, Val Loss: 2.5954, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 638/20000, Tr Loss: 0.3555, Tr Acc: 0.8458, Val Loss: 2.5905, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 639/20000, Tr Loss: 0.3697, Tr Acc: 0.8109, Val Loss: 2.5901, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 640/20000, Tr Loss: 0.3100, Tr Acc: 0.8706, Val Loss: 2.5899, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 641/20000, Tr Loss: 0.3840, Tr Acc: 0.8507, Val Loss: 2.5898, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 642/20000, Tr Loss: 0.3053, Tr Acc: 0.9104, Val Loss: 2.5880, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 643/20000, Tr Loss: 0.3449, Tr Acc: 0.8706, Val Loss: 2.5819, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 644/20000, Tr Loss: 0.3174, Tr Acc: 0.8756, Val Loss: 2.5787, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 645/20000, Tr Loss: 0.3226, Tr Acc: 0.8607, Val Loss: 2.5738, Val Acc: 0.3793 , le : 0.000100\n",
      "Epoch 646/20000, Tr Loss: 0.3166, Tr Acc: 0.8806, Val Loss: 2.5716, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 647/20000, Tr Loss: 0.2733, Tr Acc: 0.8905, Val Loss: 2.5713, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 648/20000, Tr Loss: 0.3214, Tr Acc: 0.8557, Val Loss: 2.5720, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 649/20000, Tr Loss: 0.2953, Tr Acc: 0.8706, Val Loss: 2.5662, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 650/20000, Tr Loss: 0.3514, Tr Acc: 0.8458, Val Loss: 2.5669, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 651/20000, Tr Loss: 0.3913, Tr Acc: 0.8060, Val Loss: 2.5712, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 652/20000, Tr Loss: 0.3263, Tr Acc: 0.8905, Val Loss: 2.5746, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 653/20000, Tr Loss: 0.2460, Tr Acc: 0.9403, Val Loss: 2.5832, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 654/20000, Tr Loss: 0.3343, Tr Acc: 0.8706, Val Loss: 2.5935, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 655/20000, Tr Loss: 0.2827, Tr Acc: 0.9104, Val Loss: 2.6077, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 656/20000, Tr Loss: 0.3365, Tr Acc: 0.8507, Val Loss: 2.6164, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 657/20000, Tr Loss: 0.3555, Tr Acc: 0.8706, Val Loss: 2.6200, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 658/20000, Tr Loss: 0.3684, Tr Acc: 0.8308, Val Loss: 2.6213, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 659/20000, Tr Loss: 0.3698, Tr Acc: 0.8458, Val Loss: 2.6154, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 660/20000, Tr Loss: 0.3277, Tr Acc: 0.8607, Val Loss: 2.6132, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 661/20000, Tr Loss: 0.3443, Tr Acc: 0.8657, Val Loss: 2.6167, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 662/20000, Tr Loss: 0.3229, Tr Acc: 0.8657, Val Loss: 2.6182, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 663/20000, Tr Loss: 0.3335, Tr Acc: 0.8806, Val Loss: 2.6202, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 664/20000, Tr Loss: 0.3346, Tr Acc: 0.8507, Val Loss: 2.6192, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 665/20000, Tr Loss: 0.3040, Tr Acc: 0.8856, Val Loss: 2.6155, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 666/20000, Tr Loss: 0.3677, Tr Acc: 0.8507, Val Loss: 2.6121, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 667/20000, Tr Loss: 0.3136, Tr Acc: 0.8806, Val Loss: 2.6082, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 668/20000, Tr Loss: 0.3523, Tr Acc: 0.8657, Val Loss: 2.6008, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 669/20000, Tr Loss: 0.3180, Tr Acc: 0.8756, Val Loss: 2.5923, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 670/20000, Tr Loss: 0.3297, Tr Acc: 0.8657, Val Loss: 2.5869, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 671/20000, Tr Loss: 0.2832, Tr Acc: 0.8955, Val Loss: 2.5862, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 672/20000, Tr Loss: 0.2505, Tr Acc: 0.9104, Val Loss: 2.5851, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 673/20000, Tr Loss: 0.2948, Tr Acc: 0.8756, Val Loss: 2.5849, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 674/20000, Tr Loss: 0.3855, Tr Acc: 0.8159, Val Loss: 2.5796, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 675/20000, Tr Loss: 0.3658, Tr Acc: 0.8507, Val Loss: 2.5764, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 676/20000, Tr Loss: 0.3128, Tr Acc: 0.8607, Val Loss: 2.5674, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 677/20000, Tr Loss: 0.2498, Tr Acc: 0.9055, Val Loss: 2.5598, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 678/20000, Tr Loss: 0.3209, Tr Acc: 0.8706, Val Loss: 2.5549, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 679/20000, Tr Loss: 0.3201, Tr Acc: 0.8706, Val Loss: 2.5479, Val Acc: 0.3908 , le : 0.000100\n",
      "Epoch 680/20000, Tr Loss: 0.3030, Tr Acc: 0.8905, Val Loss: 2.5396, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 681/20000, Tr Loss: 0.3292, Tr Acc: 0.8557, Val Loss: 2.5307, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 682/20000, Tr Loss: 0.3116, Tr Acc: 0.8756, Val Loss: 2.5282, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 683/20000, Tr Loss: 0.3565, Tr Acc: 0.8607, Val Loss: 2.5244, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 684/20000, Tr Loss: 0.2622, Tr Acc: 0.9154, Val Loss: 2.5255, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 685/20000, Tr Loss: 0.2965, Tr Acc: 0.8756, Val Loss: 2.5264, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 686/20000, Tr Loss: 0.3254, Tr Acc: 0.8507, Val Loss: 2.5298, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 687/20000, Tr Loss: 0.3238, Tr Acc: 0.8955, Val Loss: 2.5404, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 688/20000, Tr Loss: 0.3088, Tr Acc: 0.8905, Val Loss: 2.5481, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 689/20000, Tr Loss: 0.2763, Tr Acc: 0.9154, Val Loss: 2.5551, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 690/20000, Tr Loss: 0.3317, Tr Acc: 0.8607, Val Loss: 2.5646, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 691/20000, Tr Loss: 0.3502, Tr Acc: 0.8408, Val Loss: 2.5663, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 692/20000, Tr Loss: 0.3258, Tr Acc: 0.8756, Val Loss: 2.5666, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 693/20000, Tr Loss: 0.3593, Tr Acc: 0.8507, Val Loss: 2.5670, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 694/20000, Tr Loss: 0.3253, Tr Acc: 0.8706, Val Loss: 2.5680, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 695/20000, Tr Loss: 0.2909, Tr Acc: 0.8756, Val Loss: 2.5720, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 696/20000, Tr Loss: 0.3258, Tr Acc: 0.8607, Val Loss: 2.5740, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 697/20000, Tr Loss: 0.3389, Tr Acc: 0.8706, Val Loss: 2.5710, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 698/20000, Tr Loss: 0.2691, Tr Acc: 0.9055, Val Loss: 2.5719, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 699/20000, Tr Loss: 0.3085, Tr Acc: 0.8806, Val Loss: 2.5735, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 700/20000, Tr Loss: 0.3206, Tr Acc: 0.8607, Val Loss: 2.5730, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 701/20000, Tr Loss: 0.3156, Tr Acc: 0.8856, Val Loss: 2.5714, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 702/20000, Tr Loss: 0.3279, Tr Acc: 0.8657, Val Loss: 2.5715, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 703/20000, Tr Loss: 0.2871, Tr Acc: 0.8806, Val Loss: 2.5699, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 704/20000, Tr Loss: 0.2593, Tr Acc: 0.8905, Val Loss: 2.5633, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 705/20000, Tr Loss: 0.3214, Tr Acc: 0.8706, Val Loss: 2.5610, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 706/20000, Tr Loss: 0.2765, Tr Acc: 0.9005, Val Loss: 2.5564, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 707/20000, Tr Loss: 0.3473, Tr Acc: 0.8706, Val Loss: 2.5527, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 708/20000, Tr Loss: 0.3287, Tr Acc: 0.8657, Val Loss: 2.5487, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 709/20000, Tr Loss: 0.3251, Tr Acc: 0.8308, Val Loss: 2.5448, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 710/20000, Tr Loss: 0.3194, Tr Acc: 0.8756, Val Loss: 2.5402, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 711/20000, Tr Loss: 0.2484, Tr Acc: 0.8905, Val Loss: 2.5388, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 712/20000, Tr Loss: 0.2784, Tr Acc: 0.8955, Val Loss: 2.5290, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 713/20000, Tr Loss: 0.2797, Tr Acc: 0.9005, Val Loss: 2.5268, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 714/20000, Tr Loss: 0.2789, Tr Acc: 0.8905, Val Loss: 2.5263, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 715/20000, Tr Loss: 0.3121, Tr Acc: 0.8557, Val Loss: 2.5305, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 716/20000, Tr Loss: 0.2937, Tr Acc: 0.8905, Val Loss: 2.5354, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 717/20000, Tr Loss: 0.3319, Tr Acc: 0.8458, Val Loss: 2.5348, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 718/20000, Tr Loss: 0.2793, Tr Acc: 0.8905, Val Loss: 2.5286, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 719/20000, Tr Loss: 0.2817, Tr Acc: 0.8955, Val Loss: 2.5310, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 720/20000, Tr Loss: 0.2690, Tr Acc: 0.8955, Val Loss: 2.5344, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 721/20000, Tr Loss: 0.2191, Tr Acc: 0.9204, Val Loss: 2.5373, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 722/20000, Tr Loss: 0.2787, Tr Acc: 0.9055, Val Loss: 2.5344, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 723/20000, Tr Loss: 0.2784, Tr Acc: 0.9055, Val Loss: 2.5369, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 724/20000, Tr Loss: 0.3057, Tr Acc: 0.8806, Val Loss: 2.5343, Val Acc: 0.4023 , le : 0.000100\n",
      "Epoch 725/20000, Tr Loss: 0.2829, Tr Acc: 0.9055, Val Loss: 2.5272, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 726/20000, Tr Loss: 0.2767, Tr Acc: 0.8706, Val Loss: 2.5206, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 727/20000, Tr Loss: 0.2891, Tr Acc: 0.8706, Val Loss: 2.5106, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 728/20000, Tr Loss: 0.2976, Tr Acc: 0.8607, Val Loss: 2.5003, Val Acc: 0.4138 , le : 0.000100\n",
      "Epoch 729/20000, Tr Loss: 0.3408, Tr Acc: 0.8706, Val Loss: 2.4902, Val Acc: 0.4253 , le : 0.000100\n",
      "Epoch 730/20000, Tr Loss: 0.2463, Tr Acc: 0.9055, Val Loss: 2.4853, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 731/20000, Tr Loss: 0.2719, Tr Acc: 0.8657, Val Loss: 2.4780, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 732/20000, Tr Loss: 0.2800, Tr Acc: 0.8756, Val Loss: 2.4686, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 733/20000, Tr Loss: 0.2870, Tr Acc: 0.8756, Val Loss: 2.4649, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 734/20000, Tr Loss: 0.2868, Tr Acc: 0.9055, Val Loss: 2.4636, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 735/20000, Tr Loss: 0.2963, Tr Acc: 0.8905, Val Loss: 2.4636, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 736/20000, Tr Loss: 0.2950, Tr Acc: 0.8657, Val Loss: 2.4632, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 737/20000, Tr Loss: 0.2902, Tr Acc: 0.8905, Val Loss: 2.4673, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 738/20000, Tr Loss: 0.3128, Tr Acc: 0.8507, Val Loss: 2.4706, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 739/20000, Tr Loss: 0.2808, Tr Acc: 0.8905, Val Loss: 2.4730, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 740/20000, Tr Loss: 0.3054, Tr Acc: 0.8856, Val Loss: 2.4766, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 741/20000, Tr Loss: 0.2920, Tr Acc: 0.8905, Val Loss: 2.4798, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 742/20000, Tr Loss: 0.2236, Tr Acc: 0.9154, Val Loss: 2.4822, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 743/20000, Tr Loss: 0.2788, Tr Acc: 0.8955, Val Loss: 2.4825, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 744/20000, Tr Loss: 0.3024, Tr Acc: 0.8756, Val Loss: 2.4775, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 745/20000, Tr Loss: 0.3105, Tr Acc: 0.8657, Val Loss: 2.4824, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 746/20000, Tr Loss: 0.2892, Tr Acc: 0.8856, Val Loss: 2.4817, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 747/20000, Tr Loss: 0.3022, Tr Acc: 0.8955, Val Loss: 2.4833, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 748/20000, Tr Loss: 0.2908, Tr Acc: 0.8806, Val Loss: 2.4793, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 749/20000, Tr Loss: 0.2238, Tr Acc: 0.9154, Val Loss: 2.4778, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 750/20000, Tr Loss: 0.2808, Tr Acc: 0.8905, Val Loss: 2.4771, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 751/20000, Tr Loss: 0.3125, Tr Acc: 0.8657, Val Loss: 2.4760, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 752/20000, Tr Loss: 0.2909, Tr Acc: 0.8706, Val Loss: 2.4802, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 753/20000, Tr Loss: 0.2678, Tr Acc: 0.9005, Val Loss: 2.4845, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 754/20000, Tr Loss: 0.2989, Tr Acc: 0.8706, Val Loss: 2.4919, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 755/20000, Tr Loss: 0.2585, Tr Acc: 0.8905, Val Loss: 2.4959, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 756/20000, Tr Loss: 0.2580, Tr Acc: 0.9055, Val Loss: 2.4993, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 757/20000, Tr Loss: 0.2803, Tr Acc: 0.8856, Val Loss: 2.5052, Val Acc: 0.4368 , le : 0.000100\n",
      "Unexpected error occurred in Neptune background thread: Killing Neptune ping thread. Your run's status will not be updated and the run will be shown as inactive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NeptunePing:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 93, in __call__\n",
      "    return FinishedApiResponseFuture(future.response())  # wait synchronously\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 200, in response\n",
      "    swagger_result = self._get_swagger_result(incoming_response)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 300, in _get_swagger_result\n",
      "    unmarshal_response(\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 353, in unmarshal_response\n",
      "    raise_on_expected(incoming_response)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 420, in raise_on_expected\n",
      "    raise make_http_exception(\n",
      "bravado.exception.HTTPNotFound: 404 \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 53, in run\n",
      "    self.work()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 76, in wrapper\n",
      "    result = func(self_, *args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/utils/ping_background_job.py\", line 66, in work\n",
      "    self._container.ping()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/metadata_containers/metadata_container.py\", line 185, in ping\n",
      "    self._backend.ping(self._id, self.container_type)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 127, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 434, in ping\n",
      "    self.leaderboard_client.api.ping(\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 95, in __call__\n",
      "    self.handle_neptune_http_errors(e.response, exception=e)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 86, in handle_neptune_http_errors\n",
      "    raise ObjectNotFound() from exception\n",
      "neptune.management.exceptions.ObjectNotFound: Object not found. (code: 22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 758/20000, Tr Loss: 0.2707, Tr Acc: 0.9104, Val Loss: 2.5092, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 759/20000, Tr Loss: 0.2863, Tr Acc: 0.8856, Val Loss: 2.5193, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 760/20000, Tr Loss: 0.2616, Tr Acc: 0.9055, Val Loss: 2.5223, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 761/20000, Tr Loss: 0.2310, Tr Acc: 0.8955, Val Loss: 2.5289, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 762/20000, Tr Loss: 0.3028, Tr Acc: 0.8756, Val Loss: 2.5273, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 763/20000, Tr Loss: 0.2213, Tr Acc: 0.9403, Val Loss: 2.5259, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 764/20000, Tr Loss: 0.2653, Tr Acc: 0.9005, Val Loss: 2.5246, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 765/20000, Tr Loss: 0.3136, Tr Acc: 0.8806, Val Loss: 2.5219, Val Acc: 0.4253 , le : 0.000100\n",
      "Epoch 766/20000, Tr Loss: 0.2433, Tr Acc: 0.8955, Val Loss: 2.5115, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 767/20000, Tr Loss: 0.2948, Tr Acc: 0.8806, Val Loss: 2.5037, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 768/20000, Tr Loss: 0.2666, Tr Acc: 0.9005, Val Loss: 2.4936, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 769/20000, Tr Loss: 0.2834, Tr Acc: 0.8905, Val Loss: 2.4842, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 770/20000, Tr Loss: 0.2937, Tr Acc: 0.8756, Val Loss: 2.4760, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 771/20000, Tr Loss: 0.2388, Tr Acc: 0.9204, Val Loss: 2.4676, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 772/20000, Tr Loss: 0.2755, Tr Acc: 0.8408, Val Loss: 2.4643, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 773/20000, Tr Loss: 0.2844, Tr Acc: 0.8657, Val Loss: 2.4616, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 774/20000, Tr Loss: 0.2601, Tr Acc: 0.9104, Val Loss: 2.4622, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 775/20000, Tr Loss: 0.3144, Tr Acc: 0.8607, Val Loss: 2.4659, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 776/20000, Tr Loss: 0.2302, Tr Acc: 0.9204, Val Loss: 2.4664, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 777/20000, Tr Loss: 0.2678, Tr Acc: 0.9005, Val Loss: 2.4689, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 778/20000, Tr Loss: 0.2568, Tr Acc: 0.9154, Val Loss: 2.4721, Val Acc: 0.4483 , le : 0.000100\n",
      "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NeptuneAsyncOpProcessor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 93, in __call__\n",
      "    return FinishedApiResponseFuture(future.response())  # wait synchronously\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 200, in response\n",
      "    swagger_result = self._get_swagger_result(incoming_response)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 124, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 300, in _get_swagger_result\n",
      "    unmarshal_response(\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 353, in unmarshal_response\n",
      "    raise_on_expected(incoming_response)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/bravado/http_future.py\", line 420, in raise_on_expected\n",
      "    raise make_http_exception(\n",
      "bravado.exception.HTTPNotFound: 404 \n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 226, in run\n",
      "    super().run()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 53, in run\n",
      "    self.work()\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 242, in work\n",
      "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/threading/daemon.py\", line 76, in wrapper\n",
      "    result = func(self_, *args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/operation_processors/async_operation_processor.py\", line 255, in process_batch\n",
      "    processed_count, errors = self._processor._backend.execute_operations(\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 481, in execute_operations\n",
      "    self._execute_operations(\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/utils.py\", line 127, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/hosted_neptune_backend.py\", line 635, in _execute_operations\n",
      "    result = self.leaderboard_client.api.executeOperations(**kwargs).response().result\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 95, in __call__\n",
      "    self.handle_neptune_http_errors(e.response, exception=e)\n",
      "  File \"/home/nutapolt/.local/lib/python3.8/site-packages/neptune/new/internal/backends/swagger_client_wrapper.py\", line 86, in handle_neptune_http_errors\n",
      "    raise ObjectNotFound() from exception\n",
      "neptune.management.exceptions.ObjectNotFound: Object not found. (code: 22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 779/20000, Tr Loss: 0.2423, Tr Acc: 0.9005, Val Loss: 2.4733, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 780/20000, Tr Loss: 0.2256, Tr Acc: 0.9055, Val Loss: 2.4808, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 781/20000, Tr Loss: 0.2323, Tr Acc: 0.9254, Val Loss: 2.4856, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 782/20000, Tr Loss: 0.2972, Tr Acc: 0.8706, Val Loss: 2.4830, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 783/20000, Tr Loss: 0.2986, Tr Acc: 0.8856, Val Loss: 2.4846, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 784/20000, Tr Loss: 0.2491, Tr Acc: 0.9154, Val Loss: 2.4884, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 785/20000, Tr Loss: 0.2500, Tr Acc: 0.8856, Val Loss: 2.4866, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 786/20000, Tr Loss: 0.2416, Tr Acc: 0.9154, Val Loss: 2.4828, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 787/20000, Tr Loss: 0.3097, Tr Acc: 0.8806, Val Loss: 2.4757, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 788/20000, Tr Loss: 0.2268, Tr Acc: 0.9055, Val Loss: 2.4759, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 789/20000, Tr Loss: 0.2861, Tr Acc: 0.8955, Val Loss: 2.4690, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 790/20000, Tr Loss: 0.2389, Tr Acc: 0.9055, Val Loss: 2.4672, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 791/20000, Tr Loss: 0.1932, Tr Acc: 0.9453, Val Loss: 2.4667, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 792/20000, Tr Loss: 0.2241, Tr Acc: 0.9303, Val Loss: 2.4728, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 793/20000, Tr Loss: 0.2727, Tr Acc: 0.8955, Val Loss: 2.4719, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 794/20000, Tr Loss: 0.2760, Tr Acc: 0.8856, Val Loss: 2.4708, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 795/20000, Tr Loss: 0.2658, Tr Acc: 0.8806, Val Loss: 2.4681, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 796/20000, Tr Loss: 0.2572, Tr Acc: 0.8905, Val Loss: 2.4776, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 797/20000, Tr Loss: 0.3069, Tr Acc: 0.9055, Val Loss: 2.4818, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 798/20000, Tr Loss: 0.2466, Tr Acc: 0.9104, Val Loss: 2.4926, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 799/20000, Tr Loss: 0.2898, Tr Acc: 0.8607, Val Loss: 2.5060, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 800/20000, Tr Loss: 0.2389, Tr Acc: 0.9104, Val Loss: 2.5098, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 801/20000, Tr Loss: 0.2614, Tr Acc: 0.9104, Val Loss: 2.5152, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 802/20000, Tr Loss: 0.2398, Tr Acc: 0.9254, Val Loss: 2.5227, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 803/20000, Tr Loss: 0.2518, Tr Acc: 0.9154, Val Loss: 2.5264, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 804/20000, Tr Loss: 0.2148, Tr Acc: 0.9154, Val Loss: 2.5257, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 805/20000, Tr Loss: 0.2713, Tr Acc: 0.9154, Val Loss: 2.5175, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 806/20000, Tr Loss: 0.2664, Tr Acc: 0.8905, Val Loss: 2.5105, Val Acc: 0.4368 , le : 0.000100\n",
      "Epoch 807/20000, Tr Loss: 0.2430, Tr Acc: 0.9104, Val Loss: 2.5036, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 808/20000, Tr Loss: 0.2223, Tr Acc: 0.9254, Val Loss: 2.4994, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 809/20000, Tr Loss: 0.2576, Tr Acc: 0.9204, Val Loss: 2.4929, Val Acc: 0.4483 , le : 0.000100\n",
      "Epoch 810/20000, Tr Loss: 0.2469, Tr Acc: 0.9254, Val Loss: 2.4844, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 811/20000, Tr Loss: 0.2917, Tr Acc: 0.8706, Val Loss: 2.4826, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 812/20000, Tr Loss: 0.2720, Tr Acc: 0.9104, Val Loss: 2.4834, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 813/20000, Tr Loss: 0.2454, Tr Acc: 0.9005, Val Loss: 2.4795, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 814/20000, Tr Loss: 0.2717, Tr Acc: 0.8806, Val Loss: 2.4734, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 815/20000, Tr Loss: 0.2383, Tr Acc: 0.9055, Val Loss: 2.4715, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 816/20000, Tr Loss: 0.2572, Tr Acc: 0.9005, Val Loss: 2.4703, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 817/20000, Tr Loss: 0.2214, Tr Acc: 0.9353, Val Loss: 2.4681, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 818/20000, Tr Loss: 0.2447, Tr Acc: 0.8905, Val Loss: 2.4605, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 819/20000, Tr Loss: 0.2870, Tr Acc: 0.8756, Val Loss: 2.4588, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 820/20000, Tr Loss: 0.2340, Tr Acc: 0.9254, Val Loss: 2.4615, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 821/20000, Tr Loss: 0.2616, Tr Acc: 0.8905, Val Loss: 2.4614, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 822/20000, Tr Loss: 0.2370, Tr Acc: 0.9055, Val Loss: 2.4607, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 823/20000, Tr Loss: 0.2735, Tr Acc: 0.9005, Val Loss: 2.4549, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 824/20000, Tr Loss: 0.2214, Tr Acc: 0.8955, Val Loss: 2.4480, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 825/20000, Tr Loss: 0.2620, Tr Acc: 0.9055, Val Loss: 2.4393, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 826/20000, Tr Loss: 0.2414, Tr Acc: 0.9055, Val Loss: 2.4353, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 827/20000, Tr Loss: 0.2316, Tr Acc: 0.9204, Val Loss: 2.4337, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 828/20000, Tr Loss: 0.2413, Tr Acc: 0.9204, Val Loss: 2.4371, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 829/20000, Tr Loss: 0.2413, Tr Acc: 0.8905, Val Loss: 2.4446, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 830/20000, Tr Loss: 0.2293, Tr Acc: 0.9254, Val Loss: 2.4510, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 831/20000, Tr Loss: 0.2378, Tr Acc: 0.9204, Val Loss: 2.4480, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 832/20000, Tr Loss: 0.2470, Tr Acc: 0.9055, Val Loss: 2.4493, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 833/20000, Tr Loss: 0.2381, Tr Acc: 0.9005, Val Loss: 2.4441, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 834/20000, Tr Loss: 0.2567, Tr Acc: 0.8905, Val Loss: 2.4364, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 835/20000, Tr Loss: 0.2531, Tr Acc: 0.8955, Val Loss: 2.4252, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 836/20000, Tr Loss: 0.2588, Tr Acc: 0.9055, Val Loss: 2.4202, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 837/20000, Tr Loss: 0.2811, Tr Acc: 0.8955, Val Loss: 2.4073, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 838/20000, Tr Loss: 0.2416, Tr Acc: 0.9154, Val Loss: 2.4035, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 839/20000, Tr Loss: 0.2297, Tr Acc: 0.9055, Val Loss: 2.3960, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 840/20000, Tr Loss: 0.2078, Tr Acc: 0.9154, Val Loss: 2.3965, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 841/20000, Tr Loss: 0.2567, Tr Acc: 0.9005, Val Loss: 2.3961, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 842/20000, Tr Loss: 0.2252, Tr Acc: 0.9154, Val Loss: 2.3937, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 843/20000, Tr Loss: 0.2364, Tr Acc: 0.9104, Val Loss: 2.3882, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 844/20000, Tr Loss: 0.2645, Tr Acc: 0.9005, Val Loss: 2.3895, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 845/20000, Tr Loss: 0.2617, Tr Acc: 0.9055, Val Loss: 2.3891, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 846/20000, Tr Loss: 0.2346, Tr Acc: 0.9104, Val Loss: 2.3879, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 847/20000, Tr Loss: 0.2978, Tr Acc: 0.8756, Val Loss: 2.3829, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 848/20000, Tr Loss: 0.2506, Tr Acc: 0.8955, Val Loss: 2.3803, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 849/20000, Tr Loss: 0.2531, Tr Acc: 0.9055, Val Loss: 2.3904, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 850/20000, Tr Loss: 0.2713, Tr Acc: 0.9055, Val Loss: 2.3921, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 851/20000, Tr Loss: 0.2541, Tr Acc: 0.8955, Val Loss: 2.3887, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 852/20000, Tr Loss: 0.2008, Tr Acc: 0.9353, Val Loss: 2.3929, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 853/20000, Tr Loss: 0.2105, Tr Acc: 0.9204, Val Loss: 2.3953, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 854/20000, Tr Loss: 0.2519, Tr Acc: 0.9005, Val Loss: 2.3953, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 855/20000, Tr Loss: 0.2233, Tr Acc: 0.9254, Val Loss: 2.3926, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 856/20000, Tr Loss: 0.2367, Tr Acc: 0.9055, Val Loss: 2.3955, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 857/20000, Tr Loss: 0.2226, Tr Acc: 0.9104, Val Loss: 2.3992, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 858/20000, Tr Loss: 0.2406, Tr Acc: 0.9104, Val Loss: 2.4048, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 859/20000, Tr Loss: 0.2258, Tr Acc: 0.9204, Val Loss: 2.4101, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 860/20000, Tr Loss: 0.2326, Tr Acc: 0.9104, Val Loss: 2.4168, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 861/20000, Tr Loss: 0.2861, Tr Acc: 0.9154, Val Loss: 2.4233, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 862/20000, Tr Loss: 0.2549, Tr Acc: 0.9005, Val Loss: 2.4280, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 863/20000, Tr Loss: 0.2513, Tr Acc: 0.8856, Val Loss: 2.4250, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 864/20000, Tr Loss: 0.2316, Tr Acc: 0.9104, Val Loss: 2.4268, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 865/20000, Tr Loss: 0.2614, Tr Acc: 0.8905, Val Loss: 2.4220, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 866/20000, Tr Loss: 0.2498, Tr Acc: 0.8856, Val Loss: 2.4184, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 867/20000, Tr Loss: 0.2651, Tr Acc: 0.8706, Val Loss: 2.4094, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 868/20000, Tr Loss: 0.2381, Tr Acc: 0.8955, Val Loss: 2.4021, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 869/20000, Tr Loss: 0.2344, Tr Acc: 0.9055, Val Loss: 2.3950, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 870/20000, Tr Loss: 0.2395, Tr Acc: 0.9204, Val Loss: 2.3916, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 871/20000, Tr Loss: 0.2334, Tr Acc: 0.9154, Val Loss: 2.3941, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 872/20000, Tr Loss: 0.2479, Tr Acc: 0.8905, Val Loss: 2.3928, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 873/20000, Tr Loss: 0.2199, Tr Acc: 0.9204, Val Loss: 2.3938, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 874/20000, Tr Loss: 0.2406, Tr Acc: 0.8856, Val Loss: 2.3937, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 875/20000, Tr Loss: 0.2003, Tr Acc: 0.9403, Val Loss: 2.3981, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 876/20000, Tr Loss: 0.2437, Tr Acc: 0.9005, Val Loss: 2.3990, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 877/20000, Tr Loss: 0.2493, Tr Acc: 0.8856, Val Loss: 2.3971, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 878/20000, Tr Loss: 0.1740, Tr Acc: 0.9403, Val Loss: 2.3980, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 879/20000, Tr Loss: 0.2081, Tr Acc: 0.9254, Val Loss: 2.4012, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 880/20000, Tr Loss: 0.2576, Tr Acc: 0.8905, Val Loss: 2.3994, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 881/20000, Tr Loss: 0.1660, Tr Acc: 0.9403, Val Loss: 2.3973, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 882/20000, Tr Loss: 0.1851, Tr Acc: 0.9552, Val Loss: 2.3969, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 883/20000, Tr Loss: 0.2282, Tr Acc: 0.9254, Val Loss: 2.3979, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 884/20000, Tr Loss: 0.2645, Tr Acc: 0.8856, Val Loss: 2.3934, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 885/20000, Tr Loss: 0.2830, Tr Acc: 0.8856, Val Loss: 2.3913, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 886/20000, Tr Loss: 0.2301, Tr Acc: 0.9154, Val Loss: 2.3942, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 887/20000, Tr Loss: 0.2162, Tr Acc: 0.9303, Val Loss: 2.3947, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 888/20000, Tr Loss: 0.2495, Tr Acc: 0.9005, Val Loss: 2.3988, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 889/20000, Tr Loss: 0.1968, Tr Acc: 0.9403, Val Loss: 2.3989, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 890/20000, Tr Loss: 0.2142, Tr Acc: 0.9303, Val Loss: 2.4000, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 891/20000, Tr Loss: 0.2153, Tr Acc: 0.9204, Val Loss: 2.4002, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 892/20000, Tr Loss: 0.2041, Tr Acc: 0.9104, Val Loss: 2.3984, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 893/20000, Tr Loss: 0.2167, Tr Acc: 0.9254, Val Loss: 2.3941, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 894/20000, Tr Loss: 0.2264, Tr Acc: 0.9204, Val Loss: 2.3898, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 895/20000, Tr Loss: 0.2573, Tr Acc: 0.8955, Val Loss: 2.3841, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 896/20000, Tr Loss: 0.2063, Tr Acc: 0.9303, Val Loss: 2.3816, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 897/20000, Tr Loss: 0.2261, Tr Acc: 0.9055, Val Loss: 2.3804, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 898/20000, Tr Loss: 0.1933, Tr Acc: 0.9353, Val Loss: 2.3792, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 899/20000, Tr Loss: 0.2039, Tr Acc: 0.9303, Val Loss: 2.3793, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 900/20000, Tr Loss: 0.2060, Tr Acc: 0.9303, Val Loss: 2.3784, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 901/20000, Tr Loss: 0.1913, Tr Acc: 0.9204, Val Loss: 2.3797, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 902/20000, Tr Loss: 0.2329, Tr Acc: 0.9303, Val Loss: 2.3870, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 903/20000, Tr Loss: 0.1972, Tr Acc: 0.9254, Val Loss: 2.3973, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 904/20000, Tr Loss: 0.2377, Tr Acc: 0.8955, Val Loss: 2.4013, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 905/20000, Tr Loss: 0.2233, Tr Acc: 0.9303, Val Loss: 2.4043, Val Acc: 0.4598 , le : 0.000100\n",
      "Epoch 906/20000, Tr Loss: 0.1734, Tr Acc: 0.9303, Val Loss: 2.4089, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 907/20000, Tr Loss: 0.1753, Tr Acc: 0.9502, Val Loss: 2.4093, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 908/20000, Tr Loss: 0.1860, Tr Acc: 0.9502, Val Loss: 2.4083, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 909/20000, Tr Loss: 0.2157, Tr Acc: 0.9154, Val Loss: 2.4049, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 910/20000, Tr Loss: 0.2154, Tr Acc: 0.9254, Val Loss: 2.4005, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 911/20000, Tr Loss: 0.2294, Tr Acc: 0.9055, Val Loss: 2.4020, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 912/20000, Tr Loss: 0.2216, Tr Acc: 0.9104, Val Loss: 2.4047, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 913/20000, Tr Loss: 0.2151, Tr Acc: 0.9254, Val Loss: 2.4046, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 914/20000, Tr Loss: 0.1934, Tr Acc: 0.9403, Val Loss: 2.4050, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 915/20000, Tr Loss: 0.2278, Tr Acc: 0.9104, Val Loss: 2.4067, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 916/20000, Tr Loss: 0.2363, Tr Acc: 0.9104, Val Loss: 2.4078, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 917/20000, Tr Loss: 0.1871, Tr Acc: 0.9403, Val Loss: 2.4050, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 918/20000, Tr Loss: 0.1800, Tr Acc: 0.9453, Val Loss: 2.4047, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 919/20000, Tr Loss: 0.1880, Tr Acc: 0.9403, Val Loss: 2.4112, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 920/20000, Tr Loss: 0.1875, Tr Acc: 0.9353, Val Loss: 2.4076, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 921/20000, Tr Loss: 0.1859, Tr Acc: 0.9254, Val Loss: 2.4061, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 922/20000, Tr Loss: 0.2454, Tr Acc: 0.9005, Val Loss: 2.3937, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 923/20000, Tr Loss: 0.1961, Tr Acc: 0.9254, Val Loss: 2.3834, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 924/20000, Tr Loss: 0.1988, Tr Acc: 0.9453, Val Loss: 2.3752, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 925/20000, Tr Loss: 0.2049, Tr Acc: 0.9204, Val Loss: 2.3674, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 926/20000, Tr Loss: 0.2012, Tr Acc: 0.9254, Val Loss: 2.3649, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 927/20000, Tr Loss: 0.1842, Tr Acc: 0.9403, Val Loss: 2.3622, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 928/20000, Tr Loss: 0.2492, Tr Acc: 0.9005, Val Loss: 2.3655, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 929/20000, Tr Loss: 0.2047, Tr Acc: 0.9353, Val Loss: 2.3678, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 930/20000, Tr Loss: 0.1679, Tr Acc: 0.9254, Val Loss: 2.3715, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 931/20000, Tr Loss: 0.1956, Tr Acc: 0.9254, Val Loss: 2.3768, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 932/20000, Tr Loss: 0.1951, Tr Acc: 0.9353, Val Loss: 2.3838, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 933/20000, Tr Loss: 0.2032, Tr Acc: 0.9254, Val Loss: 2.3885, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 934/20000, Tr Loss: 0.2190, Tr Acc: 0.9005, Val Loss: 2.3953, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 935/20000, Tr Loss: 0.2498, Tr Acc: 0.9154, Val Loss: 2.4001, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 936/20000, Tr Loss: 0.2248, Tr Acc: 0.9005, Val Loss: 2.3922, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 937/20000, Tr Loss: 0.2169, Tr Acc: 0.9055, Val Loss: 2.3755, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 938/20000, Tr Loss: 0.1546, Tr Acc: 0.9652, Val Loss: 2.3639, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 939/20000, Tr Loss: 0.1657, Tr Acc: 0.9552, Val Loss: 2.3542, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 940/20000, Tr Loss: 0.1895, Tr Acc: 0.9353, Val Loss: 2.3490, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 941/20000, Tr Loss: 0.1759, Tr Acc: 0.9403, Val Loss: 2.3440, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 942/20000, Tr Loss: 0.1858, Tr Acc: 0.9303, Val Loss: 2.3485, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 943/20000, Tr Loss: 0.2138, Tr Acc: 0.8955, Val Loss: 2.3535, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 944/20000, Tr Loss: 0.1693, Tr Acc: 0.9254, Val Loss: 2.3546, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 945/20000, Tr Loss: 0.1815, Tr Acc: 0.9403, Val Loss: 2.3592, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 946/20000, Tr Loss: 0.1963, Tr Acc: 0.9204, Val Loss: 2.3668, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 947/20000, Tr Loss: 0.2309, Tr Acc: 0.9204, Val Loss: 2.3666, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 948/20000, Tr Loss: 0.1843, Tr Acc: 0.9254, Val Loss: 2.3722, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 949/20000, Tr Loss: 0.1923, Tr Acc: 0.9453, Val Loss: 2.3737, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 950/20000, Tr Loss: 0.2430, Tr Acc: 0.9005, Val Loss: 2.3850, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 951/20000, Tr Loss: 0.1619, Tr Acc: 0.9552, Val Loss: 2.3948, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 952/20000, Tr Loss: 0.1714, Tr Acc: 0.9254, Val Loss: 2.3987, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 953/20000, Tr Loss: 0.1641, Tr Acc: 0.9502, Val Loss: 2.4043, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 954/20000, Tr Loss: 0.2285, Tr Acc: 0.9254, Val Loss: 2.4024, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 955/20000, Tr Loss: 0.1938, Tr Acc: 0.9254, Val Loss: 2.3977, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 956/20000, Tr Loss: 0.1996, Tr Acc: 0.9552, Val Loss: 2.3999, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 957/20000, Tr Loss: 0.1681, Tr Acc: 0.9552, Val Loss: 2.3990, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 958/20000, Tr Loss: 0.1755, Tr Acc: 0.9552, Val Loss: 2.3976, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 959/20000, Tr Loss: 0.1789, Tr Acc: 0.9403, Val Loss: 2.3961, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 960/20000, Tr Loss: 0.2030, Tr Acc: 0.9353, Val Loss: 2.3974, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 961/20000, Tr Loss: 0.1902, Tr Acc: 0.9353, Val Loss: 2.4015, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 962/20000, Tr Loss: 0.1940, Tr Acc: 0.9303, Val Loss: 2.3981, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 963/20000, Tr Loss: 0.1953, Tr Acc: 0.9303, Val Loss: 2.3901, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 964/20000, Tr Loss: 0.1759, Tr Acc: 0.9403, Val Loss: 2.3838, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 965/20000, Tr Loss: 0.2210, Tr Acc: 0.9303, Val Loss: 2.3754, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 966/20000, Tr Loss: 0.2151, Tr Acc: 0.9055, Val Loss: 2.3669, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 967/20000, Tr Loss: 0.2178, Tr Acc: 0.9154, Val Loss: 2.3522, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 968/20000, Tr Loss: 0.2216, Tr Acc: 0.9204, Val Loss: 2.3456, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 969/20000, Tr Loss: 0.1894, Tr Acc: 0.9254, Val Loss: 2.3432, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 970/20000, Tr Loss: 0.1513, Tr Acc: 0.9602, Val Loss: 2.3464, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 971/20000, Tr Loss: 0.2342, Tr Acc: 0.9353, Val Loss: 2.3575, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 972/20000, Tr Loss: 0.2304, Tr Acc: 0.9104, Val Loss: 2.3715, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 973/20000, Tr Loss: 0.1903, Tr Acc: 0.9005, Val Loss: 2.3888, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 974/20000, Tr Loss: 0.1414, Tr Acc: 0.9552, Val Loss: 2.4054, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 975/20000, Tr Loss: 0.2126, Tr Acc: 0.9353, Val Loss: 2.4247, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 976/20000, Tr Loss: 0.1777, Tr Acc: 0.9453, Val Loss: 2.4394, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 977/20000, Tr Loss: 0.2466, Tr Acc: 0.8905, Val Loss: 2.4578, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 978/20000, Tr Loss: 0.2035, Tr Acc: 0.9154, Val Loss: 2.4638, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 979/20000, Tr Loss: 0.1963, Tr Acc: 0.9303, Val Loss: 2.4733, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 980/20000, Tr Loss: 0.2133, Tr Acc: 0.9055, Val Loss: 2.4804, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 981/20000, Tr Loss: 0.1769, Tr Acc: 0.9303, Val Loss: 2.4857, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 982/20000, Tr Loss: 0.1842, Tr Acc: 0.9254, Val Loss: 2.4817, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 983/20000, Tr Loss: 0.2042, Tr Acc: 0.9104, Val Loss: 2.4685, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 984/20000, Tr Loss: 0.2155, Tr Acc: 0.9104, Val Loss: 2.4533, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 985/20000, Tr Loss: 0.1715, Tr Acc: 0.9453, Val Loss: 2.4396, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 986/20000, Tr Loss: 0.1613, Tr Acc: 0.9502, Val Loss: 2.4214, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 987/20000, Tr Loss: 0.1847, Tr Acc: 0.9303, Val Loss: 2.4113, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 988/20000, Tr Loss: 0.2147, Tr Acc: 0.9353, Val Loss: 2.4098, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 989/20000, Tr Loss: 0.1905, Tr Acc: 0.9303, Val Loss: 2.4045, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 990/20000, Tr Loss: 0.1746, Tr Acc: 0.9403, Val Loss: 2.3960, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 991/20000, Tr Loss: 0.1846, Tr Acc: 0.9303, Val Loss: 2.3915, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 992/20000, Tr Loss: 0.1444, Tr Acc: 0.9602, Val Loss: 2.3910, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 993/20000, Tr Loss: 0.2042, Tr Acc: 0.9303, Val Loss: 2.3908, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 994/20000, Tr Loss: 0.1895, Tr Acc: 0.9254, Val Loss: 2.3912, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 995/20000, Tr Loss: 0.1566, Tr Acc: 0.9552, Val Loss: 2.3930, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 996/20000, Tr Loss: 0.1857, Tr Acc: 0.9353, Val Loss: 2.3963, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 997/20000, Tr Loss: 0.1799, Tr Acc: 0.9453, Val Loss: 2.3995, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 998/20000, Tr Loss: 0.2052, Tr Acc: 0.9154, Val Loss: 2.3953, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 999/20000, Tr Loss: 0.1564, Tr Acc: 0.9453, Val Loss: 2.3902, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1000/20000, Tr Loss: 0.1806, Tr Acc: 0.9303, Val Loss: 2.3882, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1001/20000, Tr Loss: 0.1677, Tr Acc: 0.9502, Val Loss: 2.3862, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1002/20000, Tr Loss: 0.1924, Tr Acc: 0.9254, Val Loss: 2.3813, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1003/20000, Tr Loss: 0.1387, Tr Acc: 0.9701, Val Loss: 2.3722, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1004/20000, Tr Loss: 0.2020, Tr Acc: 0.9353, Val Loss: 2.3645, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1005/20000, Tr Loss: 0.2221, Tr Acc: 0.9204, Val Loss: 2.3594, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1006/20000, Tr Loss: 0.1530, Tr Acc: 0.9453, Val Loss: 2.3529, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1007/20000, Tr Loss: 0.1720, Tr Acc: 0.9353, Val Loss: 2.3473, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1008/20000, Tr Loss: 0.1967, Tr Acc: 0.9353, Val Loss: 2.3530, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1009/20000, Tr Loss: 0.1914, Tr Acc: 0.9353, Val Loss: 2.3603, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1010/20000, Tr Loss: 0.1426, Tr Acc: 0.9502, Val Loss: 2.3691, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1011/20000, Tr Loss: 0.1784, Tr Acc: 0.9303, Val Loss: 2.3769, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1012/20000, Tr Loss: 0.1519, Tr Acc: 0.9353, Val Loss: 2.3858, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1013/20000, Tr Loss: 0.1735, Tr Acc: 0.9303, Val Loss: 2.3957, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1014/20000, Tr Loss: 0.1739, Tr Acc: 0.9453, Val Loss: 2.4010, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1015/20000, Tr Loss: 0.1396, Tr Acc: 0.9701, Val Loss: 2.4117, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1016/20000, Tr Loss: 0.1817, Tr Acc: 0.9254, Val Loss: 2.4151, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1017/20000, Tr Loss: 0.1980, Tr Acc: 0.9254, Val Loss: 2.4117, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1018/20000, Tr Loss: 0.2274, Tr Acc: 0.9254, Val Loss: 2.4132, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1019/20000, Tr Loss: 0.1506, Tr Acc: 0.9502, Val Loss: 2.4138, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1020/20000, Tr Loss: 0.1755, Tr Acc: 0.9353, Val Loss: 2.4133, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1021/20000, Tr Loss: 0.1550, Tr Acc: 0.9502, Val Loss: 2.4186, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1022/20000, Tr Loss: 0.1138, Tr Acc: 0.9751, Val Loss: 2.4151, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1023/20000, Tr Loss: 0.1861, Tr Acc: 0.9502, Val Loss: 2.4106, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1024/20000, Tr Loss: 0.1809, Tr Acc: 0.9204, Val Loss: 2.4010, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1025/20000, Tr Loss: 0.1688, Tr Acc: 0.9453, Val Loss: 2.3891, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1026/20000, Tr Loss: 0.1719, Tr Acc: 0.9353, Val Loss: 2.3854, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1027/20000, Tr Loss: 0.1837, Tr Acc: 0.9353, Val Loss: 2.3746, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1028/20000, Tr Loss: 0.1600, Tr Acc: 0.9453, Val Loss: 2.3706, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1029/20000, Tr Loss: 0.2088, Tr Acc: 0.9204, Val Loss: 2.3670, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1030/20000, Tr Loss: 0.1750, Tr Acc: 0.9254, Val Loss: 2.3667, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1031/20000, Tr Loss: 0.1444, Tr Acc: 0.9552, Val Loss: 2.3675, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1032/20000, Tr Loss: 0.1828, Tr Acc: 0.9453, Val Loss: 2.3618, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1033/20000, Tr Loss: 0.1276, Tr Acc: 0.9801, Val Loss: 2.3542, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1034/20000, Tr Loss: 0.1486, Tr Acc: 0.9602, Val Loss: 2.3517, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1035/20000, Tr Loss: 0.1654, Tr Acc: 0.9453, Val Loss: 2.3419, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1036/20000, Tr Loss: 0.1704, Tr Acc: 0.9453, Val Loss: 2.3371, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1037/20000, Tr Loss: 0.1934, Tr Acc: 0.9254, Val Loss: 2.3321, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1038/20000, Tr Loss: 0.1711, Tr Acc: 0.9403, Val Loss: 2.3264, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1039/20000, Tr Loss: 0.2217, Tr Acc: 0.9055, Val Loss: 2.3231, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1040/20000, Tr Loss: 0.1848, Tr Acc: 0.9254, Val Loss: 2.3185, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1041/20000, Tr Loss: 0.1574, Tr Acc: 0.9403, Val Loss: 2.3160, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1042/20000, Tr Loss: 0.1502, Tr Acc: 0.9502, Val Loss: 2.3116, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1043/20000, Tr Loss: 0.1621, Tr Acc: 0.9552, Val Loss: 2.3036, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1044/20000, Tr Loss: 0.1554, Tr Acc: 0.9502, Val Loss: 2.2971, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1045/20000, Tr Loss: 0.1717, Tr Acc: 0.9453, Val Loss: 2.2946, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1046/20000, Tr Loss: 0.2019, Tr Acc: 0.9403, Val Loss: 2.2859, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1047/20000, Tr Loss: 0.1583, Tr Acc: 0.9552, Val Loss: 2.2795, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1048/20000, Tr Loss: 0.1729, Tr Acc: 0.9353, Val Loss: 2.2772, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1049/20000, Tr Loss: 0.1655, Tr Acc: 0.9453, Val Loss: 2.2778, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1050/20000, Tr Loss: 0.1314, Tr Acc: 0.9602, Val Loss: 2.2828, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1051/20000, Tr Loss: 0.2330, Tr Acc: 0.9154, Val Loss: 2.2846, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1052/20000, Tr Loss: 0.1206, Tr Acc: 0.9701, Val Loss: 2.2911, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1053/20000, Tr Loss: 0.1813, Tr Acc: 0.9303, Val Loss: 2.2992, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1054/20000, Tr Loss: 0.1595, Tr Acc: 0.9353, Val Loss: 2.3106, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1055/20000, Tr Loss: 0.1845, Tr Acc: 0.9502, Val Loss: 2.3259, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1056/20000, Tr Loss: 0.1505, Tr Acc: 0.9453, Val Loss: 2.3295, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1057/20000, Tr Loss: 0.1768, Tr Acc: 0.9403, Val Loss: 2.3380, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1058/20000, Tr Loss: 0.1805, Tr Acc: 0.9403, Val Loss: 2.3421, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1059/20000, Tr Loss: 0.1652, Tr Acc: 0.9353, Val Loss: 2.3382, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1060/20000, Tr Loss: 0.1451, Tr Acc: 0.9552, Val Loss: 2.3353, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1061/20000, Tr Loss: 0.1838, Tr Acc: 0.9204, Val Loss: 2.3331, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1062/20000, Tr Loss: 0.1570, Tr Acc: 0.9602, Val Loss: 2.3358, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1063/20000, Tr Loss: 0.1450, Tr Acc: 0.9502, Val Loss: 2.3349, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1064/20000, Tr Loss: 0.1526, Tr Acc: 0.9453, Val Loss: 2.3359, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1065/20000, Tr Loss: 0.1697, Tr Acc: 0.9303, Val Loss: 2.3368, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1066/20000, Tr Loss: 0.1781, Tr Acc: 0.9403, Val Loss: 2.3383, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1067/20000, Tr Loss: 0.1730, Tr Acc: 0.9502, Val Loss: 2.3386, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1068/20000, Tr Loss: 0.1743, Tr Acc: 0.9453, Val Loss: 2.3387, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1069/20000, Tr Loss: 0.1938, Tr Acc: 0.9254, Val Loss: 2.3358, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1070/20000, Tr Loss: 0.1801, Tr Acc: 0.9254, Val Loss: 2.3387, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1071/20000, Tr Loss: 0.1555, Tr Acc: 0.9602, Val Loss: 2.3436, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1072/20000, Tr Loss: 0.2117, Tr Acc: 0.9204, Val Loss: 2.3423, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1073/20000, Tr Loss: 0.1389, Tr Acc: 0.9552, Val Loss: 2.3377, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1074/20000, Tr Loss: 0.1912, Tr Acc: 0.9353, Val Loss: 2.3354, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1075/20000, Tr Loss: 0.1898, Tr Acc: 0.9303, Val Loss: 2.3417, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1076/20000, Tr Loss: 0.1532, Tr Acc: 0.9552, Val Loss: 2.3388, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1077/20000, Tr Loss: 0.1484, Tr Acc: 0.9403, Val Loss: 2.3386, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1078/20000, Tr Loss: 0.1864, Tr Acc: 0.9353, Val Loss: 2.3423, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1079/20000, Tr Loss: 0.1798, Tr Acc: 0.9453, Val Loss: 2.3398, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1080/20000, Tr Loss: 0.1591, Tr Acc: 0.9353, Val Loss: 2.3407, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1081/20000, Tr Loss: 0.1534, Tr Acc: 0.9502, Val Loss: 2.3389, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1082/20000, Tr Loss: 0.1412, Tr Acc: 0.9552, Val Loss: 2.3303, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1083/20000, Tr Loss: 0.1831, Tr Acc: 0.9403, Val Loss: 2.3250, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1084/20000, Tr Loss: 0.1962, Tr Acc: 0.9303, Val Loss: 2.3219, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1085/20000, Tr Loss: 0.1745, Tr Acc: 0.9353, Val Loss: 2.3244, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1086/20000, Tr Loss: 0.1686, Tr Acc: 0.9403, Val Loss: 2.3209, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1087/20000, Tr Loss: 0.1744, Tr Acc: 0.9254, Val Loss: 2.3207, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1088/20000, Tr Loss: 0.1852, Tr Acc: 0.9403, Val Loss: 2.3207, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1089/20000, Tr Loss: 0.1256, Tr Acc: 0.9701, Val Loss: 2.3181, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1090/20000, Tr Loss: 0.1715, Tr Acc: 0.9254, Val Loss: 2.3174, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1091/20000, Tr Loss: 0.1692, Tr Acc: 0.9403, Val Loss: 2.3164, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1092/20000, Tr Loss: 0.1552, Tr Acc: 0.9552, Val Loss: 2.3160, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1093/20000, Tr Loss: 0.1604, Tr Acc: 0.9403, Val Loss: 2.3186, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1094/20000, Tr Loss: 0.1719, Tr Acc: 0.9254, Val Loss: 2.3242, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1095/20000, Tr Loss: 0.1932, Tr Acc: 0.9204, Val Loss: 2.3260, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1096/20000, Tr Loss: 0.1366, Tr Acc: 0.9552, Val Loss: 2.3267, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1097/20000, Tr Loss: 0.1151, Tr Acc: 0.9751, Val Loss: 2.3274, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1098/20000, Tr Loss: 0.1673, Tr Acc: 0.9453, Val Loss: 2.3270, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1099/20000, Tr Loss: 0.1387, Tr Acc: 0.9652, Val Loss: 2.3247, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1100/20000, Tr Loss: 0.1582, Tr Acc: 0.9453, Val Loss: 2.3285, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1101/20000, Tr Loss: 0.1055, Tr Acc: 0.9751, Val Loss: 2.3268, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1102/20000, Tr Loss: 0.1851, Tr Acc: 0.9303, Val Loss: 2.3286, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1103/20000, Tr Loss: 0.1658, Tr Acc: 0.9453, Val Loss: 2.3297, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1104/20000, Tr Loss: 0.1654, Tr Acc: 0.9453, Val Loss: 2.3296, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1105/20000, Tr Loss: 0.1542, Tr Acc: 0.9303, Val Loss: 2.3239, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1106/20000, Tr Loss: 0.1755, Tr Acc: 0.9204, Val Loss: 2.3192, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1107/20000, Tr Loss: 0.1324, Tr Acc: 0.9602, Val Loss: 2.3083, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1108/20000, Tr Loss: 0.1670, Tr Acc: 0.9403, Val Loss: 2.3016, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1109/20000, Tr Loss: 0.1233, Tr Acc: 0.9602, Val Loss: 2.2952, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1110/20000, Tr Loss: 0.1501, Tr Acc: 0.9453, Val Loss: 2.2904, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1111/20000, Tr Loss: 0.1971, Tr Acc: 0.9204, Val Loss: 2.2917, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1112/20000, Tr Loss: 0.1878, Tr Acc: 0.9502, Val Loss: 2.3006, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1113/20000, Tr Loss: 0.1491, Tr Acc: 0.9552, Val Loss: 2.3039, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1114/20000, Tr Loss: 0.1791, Tr Acc: 0.9453, Val Loss: 2.3060, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1115/20000, Tr Loss: 0.1342, Tr Acc: 0.9502, Val Loss: 2.3062, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1116/20000, Tr Loss: 0.1476, Tr Acc: 0.9453, Val Loss: 2.3050, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1117/20000, Tr Loss: 0.1499, Tr Acc: 0.9403, Val Loss: 2.3081, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1118/20000, Tr Loss: 0.1479, Tr Acc: 0.9502, Val Loss: 2.3100, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1119/20000, Tr Loss: 0.1518, Tr Acc: 0.9552, Val Loss: 2.3086, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1120/20000, Tr Loss: 0.1274, Tr Acc: 0.9701, Val Loss: 2.3054, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1121/20000, Tr Loss: 0.1655, Tr Acc: 0.9303, Val Loss: 2.3084, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1122/20000, Tr Loss: 0.1189, Tr Acc: 0.9801, Val Loss: 2.3035, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1123/20000, Tr Loss: 0.1061, Tr Acc: 0.9652, Val Loss: 2.3026, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1124/20000, Tr Loss: 0.1397, Tr Acc: 0.9502, Val Loss: 2.3016, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1125/20000, Tr Loss: 0.1856, Tr Acc: 0.9254, Val Loss: 2.3065, Val Acc: 0.4713 , le : 0.000100\n",
      "Epoch 1126/20000, Tr Loss: 0.1435, Tr Acc: 0.9502, Val Loss: 2.3080, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1127/20000, Tr Loss: 0.1366, Tr Acc: 0.9453, Val Loss: 2.3070, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1128/20000, Tr Loss: 0.1401, Tr Acc: 0.9502, Val Loss: 2.3029, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1129/20000, Tr Loss: 0.1714, Tr Acc: 0.9254, Val Loss: 2.3065, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1130/20000, Tr Loss: 0.1538, Tr Acc: 0.9353, Val Loss: 2.3068, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1131/20000, Tr Loss: 0.1616, Tr Acc: 0.9403, Val Loss: 2.3100, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1132/20000, Tr Loss: 0.1738, Tr Acc: 0.9204, Val Loss: 2.3128, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1133/20000, Tr Loss: 0.1290, Tr Acc: 0.9701, Val Loss: 2.3124, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1134/20000, Tr Loss: 0.1502, Tr Acc: 0.9502, Val Loss: 2.3123, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1135/20000, Tr Loss: 0.1358, Tr Acc: 0.9552, Val Loss: 2.3162, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1136/20000, Tr Loss: 0.1518, Tr Acc: 0.9403, Val Loss: 2.3127, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1137/20000, Tr Loss: 0.1467, Tr Acc: 0.9502, Val Loss: 2.3063, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1138/20000, Tr Loss: 0.1579, Tr Acc: 0.9652, Val Loss: 2.2979, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1139/20000, Tr Loss: 0.1598, Tr Acc: 0.9453, Val Loss: 2.2921, Val Acc: 0.4828 , le : 0.000100\n",
      "Epoch 1140/20000, Tr Loss: 0.1761, Tr Acc: 0.9502, Val Loss: 2.2844, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1141/20000, Tr Loss: 0.1501, Tr Acc: 0.9453, Val Loss: 2.2799, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1142/20000, Tr Loss: 0.1828, Tr Acc: 0.9303, Val Loss: 2.2774, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1143/20000, Tr Loss: 0.1215, Tr Acc: 0.9652, Val Loss: 2.2815, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1144/20000, Tr Loss: 0.1405, Tr Acc: 0.9403, Val Loss: 2.2751, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1145/20000, Tr Loss: 0.1781, Tr Acc: 0.9403, Val Loss: 2.2792, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1146/20000, Tr Loss: 0.1167, Tr Acc: 0.9652, Val Loss: 2.2887, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1147/20000, Tr Loss: 0.1878, Tr Acc: 0.9353, Val Loss: 2.2930, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1148/20000, Tr Loss: 0.1908, Tr Acc: 0.9104, Val Loss: 2.2922, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1149/20000, Tr Loss: 0.1663, Tr Acc: 0.9502, Val Loss: 2.3020, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1150/20000, Tr Loss: 0.1207, Tr Acc: 0.9801, Val Loss: 2.3074, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1151/20000, Tr Loss: 0.1123, Tr Acc: 0.9751, Val Loss: 2.3094, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1152/20000, Tr Loss: 0.1331, Tr Acc: 0.9502, Val Loss: 2.3098, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1153/20000, Tr Loss: 0.1354, Tr Acc: 0.9552, Val Loss: 2.3084, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1154/20000, Tr Loss: 0.1461, Tr Acc: 0.9453, Val Loss: 2.3074, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1155/20000, Tr Loss: 0.1201, Tr Acc: 0.9701, Val Loss: 2.3004, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1156/20000, Tr Loss: 0.1424, Tr Acc: 0.9353, Val Loss: 2.2946, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1157/20000, Tr Loss: 0.1166, Tr Acc: 0.9701, Val Loss: 2.2820, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1158/20000, Tr Loss: 0.1370, Tr Acc: 0.9502, Val Loss: 2.2720, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1159/20000, Tr Loss: 0.1176, Tr Acc: 0.9602, Val Loss: 2.2662, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1160/20000, Tr Loss: 0.1447, Tr Acc: 0.9502, Val Loss: 2.2599, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1161/20000, Tr Loss: 0.1377, Tr Acc: 0.9453, Val Loss: 2.2533, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1162/20000, Tr Loss: 0.1826, Tr Acc: 0.9403, Val Loss: 2.2409, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1163/20000, Tr Loss: 0.1243, Tr Acc: 0.9502, Val Loss: 2.2287, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1164/20000, Tr Loss: 0.1309, Tr Acc: 0.9701, Val Loss: 2.2157, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1165/20000, Tr Loss: 0.1605, Tr Acc: 0.9353, Val Loss: 2.2037, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1166/20000, Tr Loss: 0.1430, Tr Acc: 0.9502, Val Loss: 2.1912, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1167/20000, Tr Loss: 0.1359, Tr Acc: 0.9403, Val Loss: 2.1827, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1168/20000, Tr Loss: 0.1143, Tr Acc: 0.9552, Val Loss: 2.1781, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1169/20000, Tr Loss: 0.1571, Tr Acc: 0.9453, Val Loss: 2.1753, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1170/20000, Tr Loss: 0.1479, Tr Acc: 0.9353, Val Loss: 2.1680, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1171/20000, Tr Loss: 0.1549, Tr Acc: 0.9403, Val Loss: 2.1664, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1172/20000, Tr Loss: 0.1685, Tr Acc: 0.9303, Val Loss: 2.1655, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1173/20000, Tr Loss: 0.1239, Tr Acc: 0.9602, Val Loss: 2.1660, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1174/20000, Tr Loss: 0.1355, Tr Acc: 0.9602, Val Loss: 2.1699, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1175/20000, Tr Loss: 0.1160, Tr Acc: 0.9701, Val Loss: 2.1754, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1176/20000, Tr Loss: 0.1147, Tr Acc: 0.9751, Val Loss: 2.1793, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1177/20000, Tr Loss: 0.1551, Tr Acc: 0.9502, Val Loss: 2.1805, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1178/20000, Tr Loss: 0.1558, Tr Acc: 0.9353, Val Loss: 2.1831, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1179/20000, Tr Loss: 0.1248, Tr Acc: 0.9552, Val Loss: 2.1853, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1180/20000, Tr Loss: 0.1792, Tr Acc: 0.9254, Val Loss: 2.1882, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1181/20000, Tr Loss: 0.1525, Tr Acc: 0.9453, Val Loss: 2.1895, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1182/20000, Tr Loss: 0.1239, Tr Acc: 0.9652, Val Loss: 2.1904, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1183/20000, Tr Loss: 0.1721, Tr Acc: 0.9353, Val Loss: 2.1882, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1184/20000, Tr Loss: 0.1123, Tr Acc: 0.9652, Val Loss: 2.1897, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1185/20000, Tr Loss: 0.1531, Tr Acc: 0.9453, Val Loss: 2.1861, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1186/20000, Tr Loss: 0.1398, Tr Acc: 0.9652, Val Loss: 2.1846, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1187/20000, Tr Loss: 0.1424, Tr Acc: 0.9602, Val Loss: 2.1847, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1188/20000, Tr Loss: 0.1396, Tr Acc: 0.9552, Val Loss: 2.1778, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1189/20000, Tr Loss: 0.1187, Tr Acc: 0.9801, Val Loss: 2.1790, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1190/20000, Tr Loss: 0.1014, Tr Acc: 0.9701, Val Loss: 2.1775, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1191/20000, Tr Loss: 0.1094, Tr Acc: 0.9751, Val Loss: 2.1821, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1192/20000, Tr Loss: 0.1512, Tr Acc: 0.9453, Val Loss: 2.1749, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1193/20000, Tr Loss: 0.1500, Tr Acc: 0.9403, Val Loss: 2.1720, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1194/20000, Tr Loss: 0.1348, Tr Acc: 0.9701, Val Loss: 2.1598, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1195/20000, Tr Loss: 0.1169, Tr Acc: 0.9602, Val Loss: 2.1546, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1196/20000, Tr Loss: 0.1348, Tr Acc: 0.9602, Val Loss: 2.1531, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1197/20000, Tr Loss: 0.1347, Tr Acc: 0.9552, Val Loss: 2.1433, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1198/20000, Tr Loss: 0.1306, Tr Acc: 0.9701, Val Loss: 2.1370, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1199/20000, Tr Loss: 0.1745, Tr Acc: 0.9353, Val Loss: 2.1301, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1200/20000, Tr Loss: 0.1231, Tr Acc: 0.9602, Val Loss: 2.1228, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1201/20000, Tr Loss: 0.1147, Tr Acc: 0.9502, Val Loss: 2.1241, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1202/20000, Tr Loss: 0.1122, Tr Acc: 0.9801, Val Loss: 2.1303, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1203/20000, Tr Loss: 0.1004, Tr Acc: 0.9701, Val Loss: 2.1347, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1204/20000, Tr Loss: 0.1285, Tr Acc: 0.9751, Val Loss: 2.1437, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1205/20000, Tr Loss: 0.1298, Tr Acc: 0.9502, Val Loss: 2.1513, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1206/20000, Tr Loss: 0.1072, Tr Acc: 0.9751, Val Loss: 2.1622, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1207/20000, Tr Loss: 0.1112, Tr Acc: 0.9801, Val Loss: 2.1743, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1208/20000, Tr Loss: 0.1260, Tr Acc: 0.9552, Val Loss: 2.1813, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1209/20000, Tr Loss: 0.1064, Tr Acc: 0.9801, Val Loss: 2.1876, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1210/20000, Tr Loss: 0.1243, Tr Acc: 0.9453, Val Loss: 2.1931, Val Acc: 0.4943 , le : 0.000100\n",
      "Epoch 1211/20000, Tr Loss: 0.1199, Tr Acc: 0.9652, Val Loss: 2.1986, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1212/20000, Tr Loss: 0.1333, Tr Acc: 0.9552, Val Loss: 2.2062, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1213/20000, Tr Loss: 0.1707, Tr Acc: 0.9353, Val Loss: 2.2089, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1214/20000, Tr Loss: 0.1211, Tr Acc: 0.9552, Val Loss: 2.2086, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1215/20000, Tr Loss: 0.1009, Tr Acc: 0.9701, Val Loss: 2.2131, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1216/20000, Tr Loss: 0.1129, Tr Acc: 0.9652, Val Loss: 2.2190, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1217/20000, Tr Loss: 0.1159, Tr Acc: 0.9652, Val Loss: 2.2284, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1218/20000, Tr Loss: 0.1142, Tr Acc: 0.9552, Val Loss: 2.2370, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1219/20000, Tr Loss: 0.0999, Tr Acc: 0.9801, Val Loss: 2.2464, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1220/20000, Tr Loss: 0.1086, Tr Acc: 0.9652, Val Loss: 2.2431, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1221/20000, Tr Loss: 0.1476, Tr Acc: 0.9502, Val Loss: 2.2366, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1222/20000, Tr Loss: 0.1089, Tr Acc: 0.9801, Val Loss: 2.2291, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1223/20000, Tr Loss: 0.1247, Tr Acc: 0.9602, Val Loss: 2.2240, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1224/20000, Tr Loss: 0.1446, Tr Acc: 0.9502, Val Loss: 2.2163, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1225/20000, Tr Loss: 0.1119, Tr Acc: 0.9751, Val Loss: 2.2106, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1226/20000, Tr Loss: 0.1279, Tr Acc: 0.9652, Val Loss: 2.2074, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1227/20000, Tr Loss: 0.1186, Tr Acc: 0.9502, Val Loss: 2.1965, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1228/20000, Tr Loss: 0.1273, Tr Acc: 0.9552, Val Loss: 2.1882, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1229/20000, Tr Loss: 0.1062, Tr Acc: 0.9602, Val Loss: 2.1806, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1230/20000, Tr Loss: 0.1491, Tr Acc: 0.9502, Val Loss: 2.1747, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1231/20000, Tr Loss: 0.1099, Tr Acc: 0.9851, Val Loss: 2.1717, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1232/20000, Tr Loss: 0.1125, Tr Acc: 0.9652, Val Loss: 2.1769, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1233/20000, Tr Loss: 0.1251, Tr Acc: 0.9502, Val Loss: 2.1808, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1234/20000, Tr Loss: 0.1107, Tr Acc: 0.9701, Val Loss: 2.1868, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1235/20000, Tr Loss: 0.1055, Tr Acc: 0.9602, Val Loss: 2.1973, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1236/20000, Tr Loss: 0.1662, Tr Acc: 0.9453, Val Loss: 2.2035, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1237/20000, Tr Loss: 0.1137, Tr Acc: 0.9652, Val Loss: 2.2086, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1238/20000, Tr Loss: 0.1672, Tr Acc: 0.9303, Val Loss: 2.2070, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1239/20000, Tr Loss: 0.1417, Tr Acc: 0.9552, Val Loss: 2.2035, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1240/20000, Tr Loss: 0.1464, Tr Acc: 0.9403, Val Loss: 2.1973, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1241/20000, Tr Loss: 0.0972, Tr Acc: 0.9751, Val Loss: 2.1938, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1242/20000, Tr Loss: 0.0940, Tr Acc: 0.9851, Val Loss: 2.1949, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1243/20000, Tr Loss: 0.1026, Tr Acc: 0.9751, Val Loss: 2.1954, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1244/20000, Tr Loss: 0.1033, Tr Acc: 0.9652, Val Loss: 2.1992, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1245/20000, Tr Loss: 0.1571, Tr Acc: 0.9403, Val Loss: 2.2001, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1246/20000, Tr Loss: 0.1091, Tr Acc: 0.9751, Val Loss: 2.2002, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1247/20000, Tr Loss: 0.0919, Tr Acc: 0.9751, Val Loss: 2.2057, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1248/20000, Tr Loss: 0.1535, Tr Acc: 0.9303, Val Loss: 2.2053, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1249/20000, Tr Loss: 0.1297, Tr Acc: 0.9502, Val Loss: 2.2043, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1250/20000, Tr Loss: 0.1138, Tr Acc: 0.9701, Val Loss: 2.2067, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1251/20000, Tr Loss: 0.0966, Tr Acc: 0.9751, Val Loss: 2.2053, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1252/20000, Tr Loss: 0.1586, Tr Acc: 0.9154, Val Loss: 2.2157, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1253/20000, Tr Loss: 0.1185, Tr Acc: 0.9602, Val Loss: 2.2209, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1254/20000, Tr Loss: 0.1282, Tr Acc: 0.9652, Val Loss: 2.2225, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1255/20000, Tr Loss: 0.1023, Tr Acc: 0.9801, Val Loss: 2.2280, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1256/20000, Tr Loss: 0.1542, Tr Acc: 0.9353, Val Loss: 2.2269, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1257/20000, Tr Loss: 0.1055, Tr Acc: 0.9751, Val Loss: 2.2287, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1258/20000, Tr Loss: 0.1054, Tr Acc: 0.9652, Val Loss: 2.2259, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1259/20000, Tr Loss: 0.1381, Tr Acc: 0.9602, Val Loss: 2.2156, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1260/20000, Tr Loss: 0.1384, Tr Acc: 0.9502, Val Loss: 2.2128, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1261/20000, Tr Loss: 0.1085, Tr Acc: 0.9502, Val Loss: 2.2048, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1262/20000, Tr Loss: 0.1136, Tr Acc: 0.9652, Val Loss: 2.1968, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1263/20000, Tr Loss: 0.1762, Tr Acc: 0.9204, Val Loss: 2.1897, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1264/20000, Tr Loss: 0.1401, Tr Acc: 0.9602, Val Loss: 2.1889, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1265/20000, Tr Loss: 0.0960, Tr Acc: 0.9801, Val Loss: 2.1910, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1266/20000, Tr Loss: 0.0895, Tr Acc: 0.9652, Val Loss: 2.1964, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1267/20000, Tr Loss: 0.1308, Tr Acc: 0.9602, Val Loss: 2.2001, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1268/20000, Tr Loss: 0.0990, Tr Acc: 0.9801, Val Loss: 2.2038, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1269/20000, Tr Loss: 0.1292, Tr Acc: 0.9552, Val Loss: 2.2058, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1270/20000, Tr Loss: 0.0987, Tr Acc: 0.9602, Val Loss: 2.2114, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1271/20000, Tr Loss: 0.1588, Tr Acc: 0.9502, Val Loss: 2.2199, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1272/20000, Tr Loss: 0.1164, Tr Acc: 0.9552, Val Loss: 2.2262, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1273/20000, Tr Loss: 0.0971, Tr Acc: 0.9701, Val Loss: 2.2345, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1274/20000, Tr Loss: 0.1093, Tr Acc: 0.9602, Val Loss: 2.2413, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1275/20000, Tr Loss: 0.1468, Tr Acc: 0.9353, Val Loss: 2.2496, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1276/20000, Tr Loss: 0.1277, Tr Acc: 0.9502, Val Loss: 2.2507, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1277/20000, Tr Loss: 0.1106, Tr Acc: 0.9602, Val Loss: 2.2542, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1278/20000, Tr Loss: 0.1358, Tr Acc: 0.9552, Val Loss: 2.2476, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1279/20000, Tr Loss: 0.1192, Tr Acc: 0.9701, Val Loss: 2.2417, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1280/20000, Tr Loss: 0.1006, Tr Acc: 0.9701, Val Loss: 2.2350, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1281/20000, Tr Loss: 0.1366, Tr Acc: 0.9453, Val Loss: 2.2228, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1282/20000, Tr Loss: 0.1255, Tr Acc: 0.9602, Val Loss: 2.2220, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1283/20000, Tr Loss: 0.1118, Tr Acc: 0.9652, Val Loss: 2.2170, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1284/20000, Tr Loss: 0.0931, Tr Acc: 0.9701, Val Loss: 2.2081, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1285/20000, Tr Loss: 0.0824, Tr Acc: 0.9801, Val Loss: 2.2068, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1286/20000, Tr Loss: 0.1381, Tr Acc: 0.9502, Val Loss: 2.1946, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1287/20000, Tr Loss: 0.1298, Tr Acc: 0.9602, Val Loss: 2.1836, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1288/20000, Tr Loss: 0.1480, Tr Acc: 0.9254, Val Loss: 2.1830, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1289/20000, Tr Loss: 0.0943, Tr Acc: 0.9602, Val Loss: 2.1861, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1290/20000, Tr Loss: 0.1063, Tr Acc: 0.9801, Val Loss: 2.1916, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1291/20000, Tr Loss: 0.1204, Tr Acc: 0.9602, Val Loss: 2.2008, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1292/20000, Tr Loss: 0.1466, Tr Acc: 0.9353, Val Loss: 2.1988, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1293/20000, Tr Loss: 0.1160, Tr Acc: 0.9652, Val Loss: 2.2012, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1294/20000, Tr Loss: 0.1164, Tr Acc: 0.9652, Val Loss: 2.2093, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1295/20000, Tr Loss: 0.1450, Tr Acc: 0.9502, Val Loss: 2.2171, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1296/20000, Tr Loss: 0.1000, Tr Acc: 0.9751, Val Loss: 2.2211, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1297/20000, Tr Loss: 0.1270, Tr Acc: 0.9701, Val Loss: 2.2286, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1298/20000, Tr Loss: 0.1028, Tr Acc: 0.9701, Val Loss: 2.2352, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1299/20000, Tr Loss: 0.1397, Tr Acc: 0.9453, Val Loss: 2.2330, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1300/20000, Tr Loss: 0.1108, Tr Acc: 0.9602, Val Loss: 2.2258, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1301/20000, Tr Loss: 0.1075, Tr Acc: 0.9701, Val Loss: 2.2210, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1302/20000, Tr Loss: 0.0985, Tr Acc: 0.9701, Val Loss: 2.2162, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1303/20000, Tr Loss: 0.1005, Tr Acc: 0.9701, Val Loss: 2.2062, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1304/20000, Tr Loss: 0.1024, Tr Acc: 0.9652, Val Loss: 2.2032, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1305/20000, Tr Loss: 0.1285, Tr Acc: 0.9353, Val Loss: 2.1944, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1306/20000, Tr Loss: 0.0919, Tr Acc: 0.9652, Val Loss: 2.1802, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1307/20000, Tr Loss: 0.0966, Tr Acc: 0.9801, Val Loss: 2.1726, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1308/20000, Tr Loss: 0.1087, Tr Acc: 0.9652, Val Loss: 2.1598, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1309/20000, Tr Loss: 0.1220, Tr Acc: 0.9652, Val Loss: 2.1457, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1310/20000, Tr Loss: 0.1292, Tr Acc: 0.9502, Val Loss: 2.1319, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1311/20000, Tr Loss: 0.1174, Tr Acc: 0.9652, Val Loss: 2.1170, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1312/20000, Tr Loss: 0.1028, Tr Acc: 0.9701, Val Loss: 2.1102, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1313/20000, Tr Loss: 0.1273, Tr Acc: 0.9701, Val Loss: 2.1048, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1314/20000, Tr Loss: 0.1270, Tr Acc: 0.9552, Val Loss: 2.0992, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1315/20000, Tr Loss: 0.1056, Tr Acc: 0.9552, Val Loss: 2.1029, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1316/20000, Tr Loss: 0.1135, Tr Acc: 0.9701, Val Loss: 2.1030, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1317/20000, Tr Loss: 0.0967, Tr Acc: 0.9701, Val Loss: 2.1078, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1318/20000, Tr Loss: 0.1352, Tr Acc: 0.9502, Val Loss: 2.1126, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1319/20000, Tr Loss: 0.1161, Tr Acc: 0.9701, Val Loss: 2.1205, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1320/20000, Tr Loss: 0.1238, Tr Acc: 0.9602, Val Loss: 2.1251, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1321/20000, Tr Loss: 0.1084, Tr Acc: 0.9602, Val Loss: 2.1381, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1322/20000, Tr Loss: 0.1463, Tr Acc: 0.9502, Val Loss: 2.1505, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1323/20000, Tr Loss: 0.1132, Tr Acc: 0.9602, Val Loss: 2.1688, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1324/20000, Tr Loss: 0.1070, Tr Acc: 0.9701, Val Loss: 2.1783, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1325/20000, Tr Loss: 0.1319, Tr Acc: 0.9502, Val Loss: 2.1835, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1326/20000, Tr Loss: 0.1066, Tr Acc: 0.9751, Val Loss: 2.1833, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1327/20000, Tr Loss: 0.1342, Tr Acc: 0.9502, Val Loss: 2.1831, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1328/20000, Tr Loss: 0.1144, Tr Acc: 0.9652, Val Loss: 2.1798, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1329/20000, Tr Loss: 0.1073, Tr Acc: 0.9652, Val Loss: 2.1761, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1330/20000, Tr Loss: 0.1717, Tr Acc: 0.9353, Val Loss: 2.1721, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1331/20000, Tr Loss: 0.0822, Tr Acc: 0.9701, Val Loss: 2.1665, Val Acc: 0.5057 , le : 0.000100\n",
      "Epoch 1332/20000, Tr Loss: 0.1182, Tr Acc: 0.9602, Val Loss: 2.1563, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1333/20000, Tr Loss: 0.1054, Tr Acc: 0.9701, Val Loss: 2.1471, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1334/20000, Tr Loss: 0.1439, Tr Acc: 0.9602, Val Loss: 2.1379, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1335/20000, Tr Loss: 0.1165, Tr Acc: 0.9701, Val Loss: 2.1271, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1336/20000, Tr Loss: 0.1207, Tr Acc: 0.9602, Val Loss: 2.1199, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1337/20000, Tr Loss: 0.0951, Tr Acc: 0.9751, Val Loss: 2.1115, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1338/20000, Tr Loss: 0.1457, Tr Acc: 0.9552, Val Loss: 2.1136, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1339/20000, Tr Loss: 0.1002, Tr Acc: 0.9701, Val Loss: 2.1219, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1340/20000, Tr Loss: 0.1023, Tr Acc: 0.9801, Val Loss: 2.1301, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1341/20000, Tr Loss: 0.0999, Tr Acc: 0.9701, Val Loss: 2.1363, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1342/20000, Tr Loss: 0.1112, Tr Acc: 0.9602, Val Loss: 2.1462, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1343/20000, Tr Loss: 0.0973, Tr Acc: 0.9652, Val Loss: 2.1574, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1344/20000, Tr Loss: 0.1021, Tr Acc: 0.9552, Val Loss: 2.1663, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1345/20000, Tr Loss: 0.1004, Tr Acc: 0.9751, Val Loss: 2.1754, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1346/20000, Tr Loss: 0.0946, Tr Acc: 0.9751, Val Loss: 2.1896, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1347/20000, Tr Loss: 0.1054, Tr Acc: 0.9701, Val Loss: 2.1948, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1348/20000, Tr Loss: 0.1212, Tr Acc: 0.9652, Val Loss: 2.1986, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1349/20000, Tr Loss: 0.1425, Tr Acc: 0.9353, Val Loss: 2.2007, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1350/20000, Tr Loss: 0.0848, Tr Acc: 0.9701, Val Loss: 2.1974, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1351/20000, Tr Loss: 0.1011, Tr Acc: 0.9552, Val Loss: 2.1947, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1352/20000, Tr Loss: 0.1310, Tr Acc: 0.9652, Val Loss: 2.1970, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1353/20000, Tr Loss: 0.0804, Tr Acc: 0.9851, Val Loss: 2.1983, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1354/20000, Tr Loss: 0.0721, Tr Acc: 0.9900, Val Loss: 2.1996, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1355/20000, Tr Loss: 0.0957, Tr Acc: 0.9701, Val Loss: 2.1912, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1356/20000, Tr Loss: 0.0845, Tr Acc: 0.9751, Val Loss: 2.1896, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1357/20000, Tr Loss: 0.1203, Tr Acc: 0.9652, Val Loss: 2.1907, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1358/20000, Tr Loss: 0.1141, Tr Acc: 0.9751, Val Loss: 2.1856, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1359/20000, Tr Loss: 0.0950, Tr Acc: 0.9801, Val Loss: 2.1810, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1360/20000, Tr Loss: 0.0790, Tr Acc: 0.9950, Val Loss: 2.1786, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1361/20000, Tr Loss: 0.1184, Tr Acc: 0.9602, Val Loss: 2.1727, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1362/20000, Tr Loss: 0.0987, Tr Acc: 0.9801, Val Loss: 2.1677, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1363/20000, Tr Loss: 0.1163, Tr Acc: 0.9602, Val Loss: 2.1661, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1364/20000, Tr Loss: 0.1247, Tr Acc: 0.9602, Val Loss: 2.1658, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1365/20000, Tr Loss: 0.0893, Tr Acc: 0.9552, Val Loss: 2.1721, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1366/20000, Tr Loss: 0.0908, Tr Acc: 0.9701, Val Loss: 2.1775, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1367/20000, Tr Loss: 0.1001, Tr Acc: 0.9652, Val Loss: 2.1892, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1368/20000, Tr Loss: 0.0980, Tr Acc: 0.9701, Val Loss: 2.1969, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1369/20000, Tr Loss: 0.0874, Tr Acc: 0.9851, Val Loss: 2.2015, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1370/20000, Tr Loss: 0.0848, Tr Acc: 0.9851, Val Loss: 2.2078, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1371/20000, Tr Loss: 0.1277, Tr Acc: 0.9403, Val Loss: 2.2141, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1372/20000, Tr Loss: 0.1081, Tr Acc: 0.9751, Val Loss: 2.2167, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1373/20000, Tr Loss: 0.0845, Tr Acc: 0.9851, Val Loss: 2.2105, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1374/20000, Tr Loss: 0.1199, Tr Acc: 0.9652, Val Loss: 2.2140, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1375/20000, Tr Loss: 0.0973, Tr Acc: 0.9701, Val Loss: 2.2242, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1376/20000, Tr Loss: 0.0854, Tr Acc: 0.9701, Val Loss: 2.2257, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1377/20000, Tr Loss: 0.0744, Tr Acc: 0.9801, Val Loss: 2.2316, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1378/20000, Tr Loss: 0.0955, Tr Acc: 0.9751, Val Loss: 2.2322, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1379/20000, Tr Loss: 0.1073, Tr Acc: 0.9652, Val Loss: 2.2350, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1380/20000, Tr Loss: 0.0913, Tr Acc: 0.9751, Val Loss: 2.2305, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1381/20000, Tr Loss: 0.0873, Tr Acc: 0.9602, Val Loss: 2.2315, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1382/20000, Tr Loss: 0.1091, Tr Acc: 0.9502, Val Loss: 2.2268, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1383/20000, Tr Loss: 0.0802, Tr Acc: 0.9900, Val Loss: 2.2197, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1384/20000, Tr Loss: 0.0767, Tr Acc: 0.9701, Val Loss: 2.2129, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1385/20000, Tr Loss: 0.1051, Tr Acc: 0.9701, Val Loss: 2.2023, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1386/20000, Tr Loss: 0.1039, Tr Acc: 0.9552, Val Loss: 2.1937, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1387/20000, Tr Loss: 0.0894, Tr Acc: 0.9751, Val Loss: 2.1895, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1388/20000, Tr Loss: 0.1143, Tr Acc: 0.9453, Val Loss: 2.1870, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1389/20000, Tr Loss: 0.0966, Tr Acc: 0.9751, Val Loss: 2.1863, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1390/20000, Tr Loss: 0.0742, Tr Acc: 0.9851, Val Loss: 2.1847, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1391/20000, Tr Loss: 0.0961, Tr Acc: 0.9801, Val Loss: 2.1848, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1392/20000, Tr Loss: 0.0907, Tr Acc: 0.9851, Val Loss: 2.1877, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1393/20000, Tr Loss: 0.1325, Tr Acc: 0.9502, Val Loss: 2.1906, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1394/20000, Tr Loss: 0.0751, Tr Acc: 0.9851, Val Loss: 2.1946, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1395/20000, Tr Loss: 0.1047, Tr Acc: 0.9652, Val Loss: 2.1925, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1396/20000, Tr Loss: 0.1015, Tr Acc: 0.9801, Val Loss: 2.1791, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1397/20000, Tr Loss: 0.0710, Tr Acc: 0.9851, Val Loss: 2.1740, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1398/20000, Tr Loss: 0.0986, Tr Acc: 0.9801, Val Loss: 2.1751, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1399/20000, Tr Loss: 0.0824, Tr Acc: 0.9801, Val Loss: 2.1779, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1400/20000, Tr Loss: 0.0925, Tr Acc: 0.9801, Val Loss: 2.1712, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1401/20000, Tr Loss: 0.1168, Tr Acc: 0.9552, Val Loss: 2.1716, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1402/20000, Tr Loss: 0.1260, Tr Acc: 0.9502, Val Loss: 2.1642, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1403/20000, Tr Loss: 0.0875, Tr Acc: 0.9751, Val Loss: 2.1604, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1404/20000, Tr Loss: 0.0776, Tr Acc: 0.9751, Val Loss: 2.1485, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1405/20000, Tr Loss: 0.0791, Tr Acc: 0.9801, Val Loss: 2.1367, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1406/20000, Tr Loss: 0.1185, Tr Acc: 0.9652, Val Loss: 2.1351, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1407/20000, Tr Loss: 0.1404, Tr Acc: 0.9403, Val Loss: 2.1224, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1408/20000, Tr Loss: 0.0899, Tr Acc: 0.9751, Val Loss: 2.1134, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1409/20000, Tr Loss: 0.0835, Tr Acc: 0.9751, Val Loss: 2.1055, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1410/20000, Tr Loss: 0.1118, Tr Acc: 0.9602, Val Loss: 2.1023, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1411/20000, Tr Loss: 0.0878, Tr Acc: 0.9801, Val Loss: 2.0978, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1412/20000, Tr Loss: 0.0735, Tr Acc: 0.9900, Val Loss: 2.0987, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1413/20000, Tr Loss: 0.1040, Tr Acc: 0.9502, Val Loss: 2.1003, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1414/20000, Tr Loss: 0.0673, Tr Acc: 0.9851, Val Loss: 2.1005, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1415/20000, Tr Loss: 0.1199, Tr Acc: 0.9602, Val Loss: 2.0983, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1416/20000, Tr Loss: 0.0958, Tr Acc: 0.9851, Val Loss: 2.0916, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1417/20000, Tr Loss: 0.0955, Tr Acc: 0.9751, Val Loss: 2.0883, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1418/20000, Tr Loss: 0.1093, Tr Acc: 0.9701, Val Loss: 2.0857, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1419/20000, Tr Loss: 0.0860, Tr Acc: 0.9801, Val Loss: 2.0780, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1420/20000, Tr Loss: 0.0951, Tr Acc: 0.9652, Val Loss: 2.0776, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1421/20000, Tr Loss: 0.1097, Tr Acc: 0.9751, Val Loss: 2.0840, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1422/20000, Tr Loss: 0.0932, Tr Acc: 0.9851, Val Loss: 2.0911, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1423/20000, Tr Loss: 0.0600, Tr Acc: 0.9950, Val Loss: 2.0984, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1424/20000, Tr Loss: 0.0835, Tr Acc: 0.9801, Val Loss: 2.1048, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1425/20000, Tr Loss: 0.1081, Tr Acc: 0.9502, Val Loss: 2.1098, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1426/20000, Tr Loss: 0.0887, Tr Acc: 0.9751, Val Loss: 2.1240, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1427/20000, Tr Loss: 0.1006, Tr Acc: 0.9552, Val Loss: 2.1295, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1428/20000, Tr Loss: 0.1149, Tr Acc: 0.9453, Val Loss: 2.1349, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1429/20000, Tr Loss: 0.0642, Tr Acc: 0.9801, Val Loss: 2.1411, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1430/20000, Tr Loss: 0.0825, Tr Acc: 0.9801, Val Loss: 2.1419, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1431/20000, Tr Loss: 0.0822, Tr Acc: 0.9751, Val Loss: 2.1392, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1432/20000, Tr Loss: 0.0842, Tr Acc: 0.9801, Val Loss: 2.1437, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1433/20000, Tr Loss: 0.1066, Tr Acc: 0.9602, Val Loss: 2.1483, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1434/20000, Tr Loss: 0.0872, Tr Acc: 0.9751, Val Loss: 2.1506, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1435/20000, Tr Loss: 0.0963, Tr Acc: 0.9701, Val Loss: 2.1489, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1436/20000, Tr Loss: 0.0814, Tr Acc: 0.9801, Val Loss: 2.1476, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1437/20000, Tr Loss: 0.0752, Tr Acc: 0.9751, Val Loss: 2.1456, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1438/20000, Tr Loss: 0.0836, Tr Acc: 0.9701, Val Loss: 2.1393, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1439/20000, Tr Loss: 0.0886, Tr Acc: 0.9751, Val Loss: 2.1318, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1440/20000, Tr Loss: 0.0875, Tr Acc: 0.9851, Val Loss: 2.1201, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1441/20000, Tr Loss: 0.0971, Tr Acc: 0.9751, Val Loss: 2.1164, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1442/20000, Tr Loss: 0.1181, Tr Acc: 0.9652, Val Loss: 2.1049, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1443/20000, Tr Loss: 0.0967, Tr Acc: 0.9851, Val Loss: 2.1011, Val Acc: 0.5172 , le : 0.000100\n",
      "Epoch 1444/20000, Tr Loss: 0.1070, Tr Acc: 0.9701, Val Loss: 2.0976, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1445/20000, Tr Loss: 0.0813, Tr Acc: 0.9900, Val Loss: 2.0949, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1446/20000, Tr Loss: 0.0731, Tr Acc: 0.9801, Val Loss: 2.1041, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1447/20000, Tr Loss: 0.0787, Tr Acc: 0.9801, Val Loss: 2.1039, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1448/20000, Tr Loss: 0.0670, Tr Acc: 0.9950, Val Loss: 2.1092, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1449/20000, Tr Loss: 0.1029, Tr Acc: 0.9751, Val Loss: 2.1159, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1450/20000, Tr Loss: 0.1042, Tr Acc: 0.9552, Val Loss: 2.1168, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1451/20000, Tr Loss: 0.0960, Tr Acc: 0.9701, Val Loss: 2.1193, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1452/20000, Tr Loss: 0.0636, Tr Acc: 0.9851, Val Loss: 2.1203, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1453/20000, Tr Loss: 0.1382, Tr Acc: 0.9453, Val Loss: 2.1191, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1454/20000, Tr Loss: 0.0849, Tr Acc: 0.9801, Val Loss: 2.1148, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1455/20000, Tr Loss: 0.1105, Tr Acc: 0.9602, Val Loss: 2.1097, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1456/20000, Tr Loss: 0.0940, Tr Acc: 0.9652, Val Loss: 2.1039, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1457/20000, Tr Loss: 0.0829, Tr Acc: 0.9602, Val Loss: 2.0986, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1458/20000, Tr Loss: 0.0839, Tr Acc: 0.9751, Val Loss: 2.0948, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1459/20000, Tr Loss: 0.0890, Tr Acc: 0.9701, Val Loss: 2.0912, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1460/20000, Tr Loss: 0.0837, Tr Acc: 0.9900, Val Loss: 2.0874, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1461/20000, Tr Loss: 0.1139, Tr Acc: 0.9502, Val Loss: 2.0863, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1462/20000, Tr Loss: 0.0726, Tr Acc: 0.9801, Val Loss: 2.0906, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1463/20000, Tr Loss: 0.0829, Tr Acc: 0.9801, Val Loss: 2.0926, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1464/20000, Tr Loss: 0.1073, Tr Acc: 0.9751, Val Loss: 2.0952, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1465/20000, Tr Loss: 0.0918, Tr Acc: 0.9652, Val Loss: 2.0941, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1466/20000, Tr Loss: 0.0856, Tr Acc: 0.9652, Val Loss: 2.1007, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1467/20000, Tr Loss: 0.1109, Tr Acc: 0.9602, Val Loss: 2.1029, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1468/20000, Tr Loss: 0.0898, Tr Acc: 0.9652, Val Loss: 2.1047, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1469/20000, Tr Loss: 0.0532, Tr Acc: 0.9900, Val Loss: 2.1090, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1470/20000, Tr Loss: 0.1129, Tr Acc: 0.9602, Val Loss: 2.1113, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1471/20000, Tr Loss: 0.0782, Tr Acc: 0.9801, Val Loss: 2.1166, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1472/20000, Tr Loss: 0.0772, Tr Acc: 0.9851, Val Loss: 2.1210, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1473/20000, Tr Loss: 0.0833, Tr Acc: 0.9701, Val Loss: 2.1223, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1474/20000, Tr Loss: 0.0732, Tr Acc: 0.9900, Val Loss: 2.1199, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1475/20000, Tr Loss: 0.1010, Tr Acc: 0.9701, Val Loss: 2.1176, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1476/20000, Tr Loss: 0.0976, Tr Acc: 0.9602, Val Loss: 2.1143, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1477/20000, Tr Loss: 0.0793, Tr Acc: 0.9851, Val Loss: 2.1137, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1478/20000, Tr Loss: 0.0954, Tr Acc: 0.9652, Val Loss: 2.1146, Val Acc: 0.5287 , le : 0.000100\n",
      "Epoch 1479/20000, Tr Loss: 0.0667, Tr Acc: 0.9900, Val Loss: 2.1120, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1480/20000, Tr Loss: 0.0714, Tr Acc: 0.9801, Val Loss: 2.1141, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1481/20000, Tr Loss: 0.0611, Tr Acc: 0.9851, Val Loss: 2.1114, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1482/20000, Tr Loss: 0.1011, Tr Acc: 0.9701, Val Loss: 2.1077, Val Acc: 0.5402 , le : 0.000100\n",
      "Epoch 1483/20000, Tr Loss: 0.1072, Tr Acc: 0.9552, Val Loss: 2.1041, Val Acc: 0.5517 , le : 0.000100\n",
      "Epoch 1484/20000, Tr Loss: 0.0997, Tr Acc: 0.9701, Val Loss: 2.1045, Val Acc: 0.5517 , le : 0.000100\n",
      "Epoch 1485/20000, Tr Loss: 0.1253, Tr Acc: 0.9453, Val Loss: 2.1030, Val Acc: 0.5517 , le : 0.000100\n",
      "Epoch 1486/20000, Tr Loss: 0.1117, Tr Acc: 0.9801, Val Loss: 2.1044, Val Acc: 0.5517 , le : 0.000100\n",
      "Epoch 1487/20000, Tr Loss: 0.0951, Tr Acc: 0.9701, Val Loss: 2.1030, Val Acc: 0.5517 , le : 0.000100\n"
     ]
    }
   ],
   "source": [
    "for subject_id in subjects:\n",
    "    run = neptune.init_run(\n",
    "    project=\"AitBrainLab/BaseLine\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhMTMyMzg0My02NzlhLTQ3N2ItYTdmMS0yNTcwNDBmM2QwM2QifQ==\",\n",
    ")\n",
    "\n",
    "\n",
    "    dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids= [subject_id])\n",
    "    preprocess(dataset, preprocessors)\n",
    "    trial_start_offset_seconds = -0.5\n",
    "    # Extract sampling frequency, check that they are same in all datasets\n",
    "    sfreq = dataset.datasets[0].raw.info['sfreq']\n",
    "    print(sfreq)\n",
    "    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
    "    # Calculate the trial start offset in samples.\n",
    "    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
    "\n",
    "    # Create windows using braindecode function for this. It needs parameters to define how\n",
    "    # trials should be used.\n",
    "    windows_dataset = create_windows_from_events(\n",
    "        dataset,\n",
    "        trial_start_offset_samples=trial_start_offset_samples,\n",
    "        trial_stop_offset_samples=0,\n",
    "        preload=True,\n",
    "    )\n",
    "    splitted = windows_dataset.split('session')\n",
    "    train_set = splitted['session_T']\n",
    "    valid_set = splitted['session_E']\n",
    "\n",
    "    input_window_samples = train_set[0][0].shape[1]\n",
    "    X_train,y_train = extrack_dataset(train_set)\n",
    "    #X_train = X_train[:, np.newaxis,:,:]\n",
    "\n",
    "    X_valid,y_valid = extrack_dataset(valid_set)\n",
    "    #X_valid = X_valid[:, np.newaxis,:,:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train,y_train, test_size=0.3,stratify=y_train)\n",
    "    label_dict = valid_set.datasets[0].windows.event_id.items()\n",
    "    labels = list(dict(sorted(list(label_dict), key=lambda kv: kv[1])).keys())\n",
    "    print('train size',X_train.shape, y_train.shape)\n",
    "    print('test size',X_test.shape, y_test.shape)\n",
    "\n",
    "    batch_size = X_train.shape[2]\n",
    "\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=batch_size)\n",
    "    test_loader = create_dataloader(X_test, y_test, batch_size=batch_size)\n",
    "    valid_loader = create_dataloader(X_valid, y_valid, batch_size=batch_size)\n",
    "    n_classes=4\n",
    "    n_chans = X_train.shape[1]\n",
    "     # For deep4 they should be:\n",
    "    lr = 1 * 0.0001\n",
    "    weight_decay = 0.5 * 0.001\n",
    "\n",
    "    params = {\"Subject number\":subject_id,\n",
    "              \"learning_rate\": lr ,\n",
    "              \"optimizer\": \"AdamW\" ,\n",
    "              \"Network\":\"Deep4Net\",\n",
    "              \"Datasets\":\"BNCI2014001\",\n",
    "              \"sfreq\":dataset.datasets[0].raw.info['sfreq'],\n",
    "              \"Class number\":n_classes,\n",
    "              \"Channel number\": train_set[0][0].shape[0],\n",
    "              \"samples point\" : X_train.shape[2]\n",
    "\n",
    "              }\n",
    "    run[\"parameters\"] = params\n",
    "\n",
    "    # model = ShallowFBCSPNet(\n",
    "    #     n_chans,\n",
    "    #     n_classes,\n",
    "    #     input_window_samples=input_window_samples,\n",
    "    #     final_conv_length='auto',\n",
    "    # )\n",
    "    model = Deep4Net(\n",
    "                n_chans,\n",
    "                n_classes, \n",
    "                input_window_samples=input_window_samples,\n",
    "                final_conv_length='auto', \n",
    "                n_filters_time=25, \n",
    "                n_filters_spat=25,\n",
    "                filter_time_length=10, \n",
    "                pool_time_length=3,\n",
    "                pool_time_stride=3,\n",
    "                n_filters_2=50, \n",
    "                filter_length_2=10, \n",
    "                n_filters_3=100,\n",
    "                filter_length_3=10, \n",
    "                n_filters_4=200, \n",
    "                filter_length_4=10,\n",
    "                first_pool_mode=\"max\", \n",
    "                later_pool_mode=\"max\", \n",
    "                drop_prob=0.5,\n",
    "                #double_time_convs=False,\n",
    "                split_first_layer=True,\n",
    "                batch_norm=True,\n",
    "                batch_norm_alpha=0.1, \n",
    "                stride_before_pool=False\n",
    "                )\n",
    "\n",
    "\n",
    "    net = model.cuda(0)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()#nn.BCELoss()#\n",
    "    train_loss,valid_loss,train_accuracy,valid_accuracy,part_weights =train(\n",
    "                                                                model = net,\n",
    "                                                                gpu_num = 0,\n",
    "                                                                train_loader = train_loader,\n",
    "                                                                test_loader = test_loader,\n",
    "                                                                optimizer = optimizer  ,\n",
    "                                                                criterion = criterion,\n",
    "                                                                num_epochs=n_epochs,\n",
    "                                                                save_weights= True,\n",
    "                                                                lr=lr\n",
    "                                                                     )\n",
    "    model.load_state_dict(torch.load(part_weights))\n",
    "    eval(model = net,\n",
    "        gpu_num = 0,\n",
    "          vail_loader= valid_loader,\n",
    "         labels=labels,\n",
    "         )\n",
    "\n",
    "\n",
    "    run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    run.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
